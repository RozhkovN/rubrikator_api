"""
–û—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –∂–∞–ª–æ–±.

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥:
1. Sentence Transformers –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
2. –ê–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è —É—Ç–æ—á–Ω–µ–Ω–∏—è
"""

import os
import pickle
import numpy as np
from typing import List, Dict, Tuple, Optional
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

from config.rubrics import RUBRICS, get_rubric_by_id
from src.preprocessor import normalize_text, calculate_keyword_score


class ComplaintClassifier:
    """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∂–∞–ª–æ–± –Ω–∞ –æ—Å–Ω–æ–≤–µ Sentence Transformers"""
    
    def __init__(
        self,
        model_name: str = "paraphrase-multilingual-mpnet-base-v2",
        use_keywords: bool = True,
        keyword_weight: float = 0.3
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞.
        
        Args:
            model_name: –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ Sentence Transformers
            use_keywords: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ –∞–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            keyword_weight: –≤–µ—Å –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ (0-1), –æ—Å—Ç–∞–ª—å–Ω–æ–µ - —Å–µ–º–∞–Ω—Ç–∏–∫–∞
        """
        self.model_name = model_name
        self.use_keywords = use_keywords
        self.keyword_weight = keyword_weight
        self.semantic_weight = 1 - keyword_weight
        
        self.model: Optional[SentenceTransformer] = None
        self.rubric_embeddings: Optional[np.ndarray] = None
        self.rubrics = RUBRICS
        
        print(f"üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞...")
        print(f"   –ú–æ–¥–µ–ª—å: {model_name}")
        print(f"   –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {'–î–∞' if use_keywords else '–ù–µ—Ç'}")
        if use_keywords:
            print(f"   –í–µ—Å–∞: —Å–µ–º–∞–Ω—Ç–∏–∫–∞={self.semantic_weight:.1f}, –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞={self.keyword_weight:.1f}")
    
    def load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Sentence Transformers"""
        if self.model is None:
            print(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {self.model_name}...")
            self.model = SentenceTransformer(self.model_name)
            print("‚úì –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    
    def prepare_rubric_texts(self) -> List[str]:
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π —Ä—É–±—Ä–∏–∫–∞—Ç–æ—Ä–æ–≤.
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
        """
        texts = []
        for rubric in self.rubrics:
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –∏ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è
            text = f"{rubric['description']}. –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {', '.join(rubric['keywords'])}"
            texts.append(text)
        return texts
    
    def train(self, save_path: str = "models/classifier.pkl"):
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ (—Å–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ä—É–±—Ä–∏–∫–∞—Ç–æ—Ä–æ–≤).
        
        Args:
            save_path: –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        """
        print("\nüéØ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å, –µ—Å–ª–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞
        self.load_model()
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ä—É–±—Ä–∏–∫–∞—Ç–æ—Ä–æ–≤
        print("üìù –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ–ø–∏—Å–∞–Ω–∏–π —Ä—É–±—Ä–∏–∫–∞—Ç–æ—Ä–æ–≤...")
        rubric_texts = self.prepare_rubric_texts()
        
        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        print("üîÑ –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π...")
        self.rubric_embeddings = self.model.encode(
            rubric_texts,
            show_progress_bar=True,
            convert_to_numpy=True
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        with open(save_path, 'wb') as f:
            pickle.dump({
                'embeddings': self.rubric_embeddings,
                'model_name': self.model_name,
                'use_keywords': self.use_keywords,
                'keyword_weight': self.keyword_weight
            }, f)
        
        print(f"‚úì –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {save_path}")
        print(f"‚úì –°–æ–∑–¥–∞–Ω–æ {len(self.rubric_embeddings)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ä—É–±—Ä–∏–∫–∞—Ç–æ—Ä–æ–≤")
    
    def load(self, load_path: str = "models/classifier.pkl"):
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞.
        
        Args:
            load_path: –ø—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        """
        print(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –∏–∑ {load_path}...")
        
        with open(load_path, 'rb') as f:
            data = pickle.load(f)
        
        self.rubric_embeddings = data['embeddings']
        self.model_name = data['model_name']
        self.use_keywords = data.get('use_keywords', True)
        self.keyword_weight = data.get('keyword_weight', 0.3)
        self.semantic_weight = 1 - self.keyword_weight
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        self.load_model()
        
        print("‚úì –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω")
    
    def _calculate_semantic_scores(self, text: str) -> np.ndarray:
        """
        –†–∞—Å—á–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Ü–µ–Ω–æ–∫ —á–µ—Ä–µ–∑ cosine similarity.
        
        Args:
            text: —Ç–µ–∫—Å—Ç –∂–∞–ª–æ–±—ã
            
        Returns:
            –ú–∞—Å—Å–∏–≤ –æ—Ü–µ–Ω–æ–∫ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä—É–±—Ä–∏–∫–∞—Ç–æ—Ä–∞
        """
        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ —Ç–µ–∫—Å—Ç–∞
        text_embedding = self.model.encode([text], convert_to_numpy=True)
        
        # –°—á–∏—Ç–∞–µ–º cosine similarity —Å–æ –≤—Å–µ–º–∏ —Ä—É–±—Ä–∏–∫–∞—Ç–æ—Ä–∞–º–∏
        similarities = cosine_similarity(text_embedding, self.rubric_embeddings)[0]
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤ –¥–∏–∞–ø–∞–∑–æ–Ω [0, 1]
        # Cosine similarity —É–∂–µ –≤ [-1, 1], –Ω–æ –æ–±—ã—á–Ω–æ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π
        scores = (similarities + 1) / 2
        
        return scores
    
    def _calculate_keyword_scores(self, text: str) -> np.ndarray:
        """
        –†–∞—Å—á–µ—Ç –æ—Ü–µ–Ω–æ–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤.
        
        Args:
            text: —Ç–µ–∫—Å—Ç –∂–∞–ª–æ–±—ã
            
        Returns:
            –ú–∞—Å—Å–∏–≤ –æ—Ü–µ–Ω–æ–∫ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä—É–±—Ä–∏–∫–∞—Ç–æ—Ä–∞
        """
        scores = np.zeros(len(self.rubrics))
        
        for i, rubric in enumerate(self.rubrics):
            scores[i] = calculate_keyword_score(text, rubric['keywords'])
        
        return scores
    
    def predict(
        self,
        text: str,
        top_k: int = 3,
        return_scores: bool = True
    ) -> Dict:
        """
        –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∂–∞–ª–æ–±—ã.
        
        Args:
            text: —Ç–µ–∫—Å—Ç –∂–∞–ª–æ–±—ã
            top_k: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            return_scores: –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å –ª–∏ –¥–µ—Ç–∞–ª—å–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        """
        if self.model is None or self.rubric_embeddings is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ load() –∏–ª–∏ train()")
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–µ–∫—Å—Ç
        text_normalized = normalize_text(text)
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –æ—Ü–µ–Ω–∫–∏
        semantic_scores = self._calculate_semantic_scores(text)
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –æ—Ü–µ–Ω–∫–∏ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        if self.use_keywords:
            keyword_scores = self._calculate_keyword_scores(text)
            # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –æ—Ü–µ–Ω–∫–∏
            final_scores = (
                self.semantic_weight * semantic_scores +
                self.keyword_weight * keyword_scores
            )
        else:
            keyword_scores = np.zeros(len(self.rubrics))
            final_scores = semantic_scores
        
        # –ù–∞—Ö–æ–¥–∏–º —Ç–æ–ø-k —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        top_indices = np.argsort(final_scores)[::-1][:top_k]
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        predictions = []
        for idx in top_indices:
            rubric = self.rubrics[idx]
            predictions.append({
                'rubric_id': rubric['id'],
                'rubric_name': rubric['description'],  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
                'short_name': rubric['name'],  # –ö—Ä–∞—Ç–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–ª—è —Å–ø—Ä–∞–≤–∫–∏
                'confidence': float(final_scores[idx]),
                'semantic_score': float(semantic_scores[idx]) if return_scores else None,
                'keyword_score': float(keyword_scores[idx]) if return_scores else None
            })
        
        result = {
            'text': text,
            'predictions': predictions,
            'best_match': predictions[0] if predictions else None
        }
        
        return result
    
    def predict_batch(
        self,
        texts: List[str],
        top_k: int = 1
    ) -> List[Dict]:
        """
        –ü–∞–∫–µ—Ç–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∂–∞–ª–æ–±.
        
        Args:
            texts: —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –∂–∞–ª–æ–±
            top_k: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        """
        results = []
        for text in texts:
            result = self.predict(text, top_k=top_k, return_scores=False)
            results.append(result)
        return results
