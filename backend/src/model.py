"""
–û—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –∂–∞–ª–æ–±.

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥:
1. Sentence Transformers –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ (—Å ONNX —É—Å–∫–æ—Ä–µ–Ω–∏–µ–º)
2. –ê–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è —É—Ç–æ—á–Ω–µ–Ω–∏—è
3. –ü—Ä–∏–º–µ—Ä—ã –∂–∞–ª–æ–± –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
4. –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –∏ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
5. –í–∑–∞–∏–º–æ–∏—Å–∫–ª—é—á–∞—é—â–∏–µ –ø—Ä–∞–≤–∏–ª–∞ –º–µ–∂–¥—É —Ä—É–±—Ä–∏–∫–∞–º–∏
6. –£–º–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤
7. LRU-–∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
"""

import os
import pickle
import hashlib
import numpy as np
from typing import List, Dict, Tuple, Optional
from functools import lru_cache
from collections import OrderedDict
import threading
import time

from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

from config.rubrics import RUBRICS, get_rubric_by_id
from config.response_templates import get_response_template
from src.preprocessor import (
    normalize_text, 
    calculate_keyword_score,
    calculate_advanced_keyword_score,
    extract_law_references,
    extract_organization_mentions
)


# ============================================================================
# LRU Cache –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
# ============================================================================

class EmbeddingCache:
    """Thread-safe LRU –∫—ç—à –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ç–µ–∫—Å—Ç–æ–≤"""
    
    def __init__(self, maxsize: int = 1000, ttl_seconds: int = 3600):
        self.maxsize = maxsize
        self.ttl_seconds = ttl_seconds
        self.cache: OrderedDict = OrderedDict()
        self.timestamps: Dict[str, float] = {}
        self.lock = threading.Lock()
        self.hits = 0
        self.misses = 0
    
    def _hash_text(self, text: str) -> str:
        """–°–æ–∑–¥–∞—ë—Ç —Ö—ç—à —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∫–ª—é—á–∞ –∫—ç—à–∞"""
        return hashlib.md5(text.encode('utf-8')).hexdigest()
    
    def get(self, text: str) -> Optional[np.ndarray]:
        """–ü–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥ –∏–∑ –∫—ç—à–∞"""
        key = self._hash_text(text)
        with self.lock:
            if key in self.cache:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º TTL
                if time.time() - self.timestamps[key] < self.ttl_seconds:
                    # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –≤ –∫–æ–Ω–µ—Ü (LRU)
                    self.cache.move_to_end(key)
                    self.hits += 1
                    return self.cache[key]
                else:
                    # TTL –∏—Å—Ç—ë–∫, —É–¥–∞–ª—è–µ–º
                    del self.cache[key]
                    del self.timestamps[key]
            self.misses += 1
            return None
    
    def set(self, text: str, embedding: np.ndarray):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥ –≤ –∫—ç—à"""
        key = self._hash_text(text)
        with self.lock:
            if key in self.cache:
                self.cache.move_to_end(key)
            else:
                if len(self.cache) >= self.maxsize:
                    # –£–¥–∞–ª—è–µ–º —Å–∞–º—ã–π —Å—Ç–∞—Ä—ã–π
                    oldest_key = next(iter(self.cache))
                    del self.cache[oldest_key]
                    del self.timestamps[oldest_key]
                self.cache[key] = embedding.copy()
            self.timestamps[key] = time.time()
    
    def clear(self):
        """–û—á–∏—Å—Ç–∏—Ç—å –∫—ç—à"""
        with self.lock:
            self.cache.clear()
            self.timestamps.clear()
            self.hits = 0
            self.misses = 0
    
    def stats(self) -> Dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—ç—à–∞"""
        with self.lock:
            total = self.hits + self.misses
            hit_rate = self.hits / total if total > 0 else 0
            return {
                'size': len(self.cache),
                'maxsize': self.maxsize,
                'hits': self.hits,
                'misses': self.misses,
                'hit_rate': f"{hit_rate:.2%}"
            }


# ============================================================================
# –í–∑–∞–∏–º–æ–∏—Å–∫–ª—é—á–∞—é—â–∏–µ –ø—Ä–∞–≤–∏–ª–∞ –º–µ–∂–¥—É —Ä—É–±—Ä–∏–∫–∞–º–∏
# ============================================================================

# –ì—Ä—É–ø–ø—ã –≤–∑–∞–∏–º–æ–∏—Å–∫–ª—é—á–∞—é—â–∏—Ö —Ä—É–±—Ä–∏–∫ (–µ—Å–ª–∏ –æ–¥–Ω–∞ –≤—ã–±—Ä–∞–Ω–∞ —Å –≤—ã—Å–æ–∫–∏–º confidence,
# –¥—Ä—É–≥–∏–µ –∏–∑ –≥—Ä—É–ø–ø—ã –ø–æ–ª—É—á–∞—é—Ç —à—Ç—Ä–∞—Ñ)
MUTUALLY_EXCLUSIVE_GROUPS = [
    # –ë–∞–Ω–∫–æ–≤—Å–∫–∏–µ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ vs –æ–±—â–∏–µ –∂–∞–ª–æ–±—ã –Ω–∞ –±–∞–Ω–∫–∏
    {1, 2, 3},  # ID: 1-–∂–∞–ª–æ–±–∞ –Ω–∞ –±–∞–Ω–∫, 2-–±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ 161-–§–ó, 3-–Ω–∞—Ä—É—à–µ–Ω–∏–µ 115-–§–ó
    
    # –§–°–°–ü vs –ö–æ–ª–ª–µ–∫—Ç–æ—Ä—ã
    {4, 5},  # ID: 4-–§–°–°–ü, 5-–∫–æ–ª–ª–µ–∫—Ç–æ—Ä—ã
    
    # –£–ø—Ä–∞–≤–ª—è—é—â–∏–π vs –ê–¥–≤–æ–∫–∞—Ç
    {8, 9},  # ID: 8-—É–ø—Ä–∞–≤–ª—è—é—â–∏–π, 9-–∞–¥–≤–æ–∫–∞—Ç
    
    # –ö–∞–∑–∏–Ω–æ: —Ä–µ–∫–≤–∏–∑–∏—Ç—ã vs –∂–∞–ª–æ–±–∞
    {10, 11},  # ID: 10-—Ä–µ–∫–≤–∏–∑–∏—Ç—ã –∫–∞–∑–∏–Ω–æ, 11-–∂–∞–ª–æ–±–∞ –Ω–∞ –∫–∞–∑–∏–Ω–æ
    
    # –ú–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ –æ—Ç –∏–º–µ–Ω–∏ –†–æ—Å—Ñ–∏–Ω–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ (—Ä–∞–∑–Ω—ã–µ —Ç–∏–ø—ã)
    {12, 13, 14, 15},  # ID: 12-–∑–≤–æ–Ω–æ–∫, 13-–¥–æ–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å, 14-–ø–∏—Å—å–º–æ, 15-–ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞
    
    # –í–∑–ª–æ–º –ì–æ—Å—É—Å–ª—É–≥ (—Å –ø–æ–ª–∏—Ü–∏–µ–π –∏ –±–µ–∑)
    {17, 18},  # ID: 17-–≤–∑–ª–æ–º, 18-–≤–∑–ª–æ–º+–ø–æ–ª–∏—Ü–∏—è
]

# –ü–æ—Ä–æ–≥–∏ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤
MIN_CONFIDENCE_THRESHOLD = 0.35  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –¥–ª—è –ø–æ–∫–∞–∑–∞ –≤–∞—Ä–∏–∞–Ω—Ç–∞
MIN_GAP_THRESHOLD = 0.15  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É –ª—É—á—à–∏–º –∏ —Å–ª–µ–¥—É—é—â–∏–º


class ComplaintClassifier:
    """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∂–∞–ª–æ–± –Ω–∞ –æ—Å–Ω–æ–≤–µ Sentence Transformers —Å ONNX —É—Å–∫–æ—Ä–µ–Ω–∏–µ–º"""
    
    def __init__(
        self,
        model_name: str = "paraphrase-multilingual-mpnet-base-v2",
        use_keywords: bool = True,
        keyword_weight: float = 0.35,
        use_examples: bool = True,
        use_onnx: bool = True,
        cache_size: int = 1000,
        cache_ttl: int = 3600
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞.
        
        Args:
            model_name: –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ Sentence Transformers
            use_keywords: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ –∞–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            keyword_weight: –≤–µ—Å –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ (0-1), –æ—Å—Ç–∞–ª—å–Ω–æ–µ - —Å–µ–º–∞–Ω—Ç–∏–∫–∞
            use_examples: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            use_onnx: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ ONNX —É—Å–∫–æ—Ä–µ–Ω–∏–µ (2-3x –±—ã—Å—Ç—Ä–µ–µ)
            cache_size: —Ä–∞–∑–º–µ—Ä LRU –∫—ç—à–∞ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            cache_ttl: –≤—Ä–µ–º—è –∂–∏–∑–Ω–∏ –∫—ç—à–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        """
        self.model_name = model_name
        self.use_keywords = use_keywords
        self.keyword_weight = keyword_weight
        self.semantic_weight = 1 - keyword_weight
        self.use_examples = use_examples
        self.use_onnx = use_onnx
        
        self.model: Optional[SentenceTransformer] = None
        self.rubric_embeddings: Optional[np.ndarray] = None
        self.example_embeddings: Optional[Dict[int, np.ndarray]] = None
        self.rubrics = RUBRICS
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫—ç—à–∞
        self.embedding_cache = EmbeddingCache(maxsize=cache_size, ttl_seconds=cache_ttl)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        self.stats = {
            'total_predictions': 0,
            'avg_time_ms': 0,
            'total_time_ms': 0
        }
        
        print(f"üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞...")
        print(f"   –ú–æ–¥–µ–ª—å: {model_name}")
        print(f"   ONNX —É—Å–∫–æ—Ä–µ–Ω–∏–µ: {'–î–∞' if use_onnx else '–ù–µ—Ç'}")
        print(f"   –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {'–î–∞' if use_keywords else '–ù–µ—Ç'}")
        print(f"   –ü—Ä–∏–º–µ—Ä—ã: {'–î–∞' if use_examples else '–ù–µ—Ç'}")
        print(f"   –ö—ç—à: {cache_size} —ç–ª–µ–º–µ–Ω—Ç–æ–≤, TTL {cache_ttl}—Å")
        if use_keywords:
            print(f"   –í–µ—Å–∞: —Å–µ–º–∞–Ω—Ç–∏–∫–∞={self.semantic_weight:.2f}, –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞={self.keyword_weight:.2f}")
    
    def load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Sentence Transformers —Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º ONNX –±—ç–∫–µ–Ω–¥–æ–º"""
        if self.model is None:
            print(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {self.model_name}...")
            
            # –ü–æ–ø—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å ONNX
            if self.use_onnx:
                try:
                    # ONNX Runtime –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
                    self.model = SentenceTransformer(
                        self.model_name,
                        backend="onnx"
                    )
                    print("‚úì –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —Å ONNX —É—Å–∫–æ—Ä–µ–Ω–∏–µ–º")
                except Exception as e:
                    print(f"‚ö†Ô∏è  ONNX –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
                    print("   –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏...")
                    self.model = SentenceTransformer(self.model_name)
                    print("‚úì –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (–±–µ–∑ ONNX)")
            else:
                self.model = SentenceTransformer(self.model_name)
                print("‚úì –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    
    def prepare_rubric_texts(self) -> List[str]:
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π —Ä—É–±—Ä–∏–∫–∞—Ç–æ—Ä–æ–≤.
        –£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏.
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
        """
        texts = []
        for rubric in self.rubrics:
            # –ë–∞–∑–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
            parts = [rubric['description']]
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            parts.append(f"–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {', '.join(rubric['keywords'])}")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            if 'priority_keywords' in rubric and rubric['priority_keywords']:
                parts.append(f"–í–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {', '.join(rubric['priority_keywords'])}")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            if self.use_examples and 'examples' in rubric and rubric['examples']:
                examples_text = " | ".join(rubric['examples'][:3])  # –ë–µ—Ä–µ–º –¥–æ 3 –ø—Ä–∏–º–µ—Ä–æ–≤
                parts.append(f"–ü—Ä–∏–º–µ—Ä—ã –æ–±—Ä–∞—â–µ–Ω–∏–π: {examples_text}")
            
            text = ". ".join(parts)
            texts.append(text)
        
        return texts
    
    def train(self, save_path: str = "models/classifier.pkl"):
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ (—Å–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ä—É–±—Ä–∏–∫–∞—Ç–æ—Ä–æ–≤).
        
        Args:
            save_path: –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        """
        print("\nüéØ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å, –µ—Å–ª–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞
        self.load_model()
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ä—É–±—Ä–∏–∫–∞—Ç–æ—Ä–æ–≤
        print("üìù –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ–ø–∏—Å–∞–Ω–∏–π —Ä—É–±—Ä–∏–∫–∞—Ç–æ—Ä–æ–≤...")
        rubric_texts = self.prepare_rubric_texts()
        
        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ä—É–±—Ä–∏–∫–∞—Ç–æ—Ä–æ–≤
        print("üîÑ –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π —Ä—É–±—Ä–∏–∫–∞—Ç–æ—Ä–æ–≤...")
        self.rubric_embeddings = self.model.encode(
            rubric_texts,
            show_progress_bar=True,
            convert_to_numpy=True
        )
        
        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä—É–±—Ä–∏–∫–∞—Ç–æ—Ä–∞
        self.example_embeddings = {}
        if self.use_examples:
            print("üîÑ –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –ø—Ä–∏–º–µ—Ä–æ–≤...")
            for rubric in self.rubrics:
                if 'examples' in rubric and rubric['examples']:
                    embeddings = self.model.encode(
                        rubric['examples'],
                        convert_to_numpy=True
                    )
                    self.example_embeddings[rubric['id']] = embeddings
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        with open(save_path, 'wb') as f:
            pickle.dump({
                'embeddings': self.rubric_embeddings,
                'example_embeddings': self.example_embeddings,
                'model_name': self.model_name,
                'use_keywords': self.use_keywords,
                'keyword_weight': self.keyword_weight,
                'use_examples': self.use_examples
            }, f)
        
        print(f"‚úì –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {save_path}")
        print(f"‚úì –°–æ–∑–¥–∞–Ω–æ {len(self.rubric_embeddings)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ä—É–±—Ä–∏–∫–∞—Ç–æ—Ä–æ–≤")
        print(f"‚úì –°–æ–∑–¥–∞–Ω–æ {len(self.example_embeddings)} –Ω–∞–±–æ—Ä–æ–≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    def load(self, load_path: str = "models/classifier.pkl"):
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞.
        
        Args:
            load_path: –ø—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        """
        print(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –∏–∑ {load_path}...")
        
        with open(load_path, 'rb') as f:
            data = pickle.load(f)
        
        self.rubric_embeddings = data['embeddings']
        self.example_embeddings = data.get('example_embeddings', {})
        self.model_name = data['model_name']
        self.use_keywords = data.get('use_keywords', True)
        self.keyword_weight = data.get('keyword_weight', 0.35)
        self.use_examples = data.get('use_examples', True)
        self.semantic_weight = 1 - self.keyword_weight
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        self.load_model()
        
        # –û—á–∏—â–∞–µ–º –∫—ç—à –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏
        self.embedding_cache.clear()
        
        print("‚úì –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω")
    
    def _get_text_embedding(self, text: str) -> np.ndarray:
        """
        –ü–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥ —Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫—ç—à–∞.
        
        Args:
            text: —Ç–µ–∫—Å—Ç –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
            
        Returns:
            –≠–º–±–µ–¥–¥–∏–Ω–≥ —Ç–µ–∫—Å—Ç–∞
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        cached = self.embedding_cache.get(text)
        if cached is not None:
            return cached
        
        # –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π —ç–º–±–µ–¥–¥–∏–Ω–≥
        embedding = self.model.encode([text], convert_to_numpy=True)[0]
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
        self.embedding_cache.set(text, embedding)
        
        return embedding
    
    def _calculate_semantic_scores(self, text: str) -> np.ndarray:
        """
        –†–∞—Å—á–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Ü–µ–Ω–æ–∫ —á–µ—Ä–µ–∑ cosine similarity.
        –£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å —É—á–µ—Ç–æ–º –ø—Ä–∏–º–µ—Ä–æ–≤ –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º.
        
        Args:
            text: —Ç–µ–∫—Å—Ç –∂–∞–ª–æ–±—ã
            
        Returns:
            –ú–∞—Å—Å–∏–≤ –æ—Ü–µ–Ω–æ–∫ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä—É–±—Ä–∏–∫–∞—Ç–æ—Ä–∞
        """
        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ —Ç–µ–∫—Å—Ç–∞ (—Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º)
        text_embedding = self._get_text_embedding(text).reshape(1, -1)
        
        # –°—á–∏—Ç–∞–µ–º cosine similarity —Å –æ–ø–∏—Å–∞–Ω–∏—è–º–∏ —Ä—É–±—Ä–∏–∫–∞—Ç–æ—Ä–æ–≤
        rubric_similarities = cosine_similarity(text_embedding, self.rubric_embeddings)[0]
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤ –¥–∏–∞–ø–∞–∑–æ–Ω [0, 1]
        scores = (rubric_similarities + 1) / 2
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏
        if self.use_examples and self.example_embeddings:
            for rubric in self.rubrics:
                rubric_id = rubric['id']
                idx = rubric_id - 1  # ID –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å 1
                
                if rubric_id in self.example_embeddings:
                    example_emb = self.example_embeddings[rubric_id]
                    example_sim = cosine_similarity(text_embedding, example_emb)[0]
                    
                    # –ë–µ—Ä–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏
                    max_example_sim = (np.max(example_sim) + 1) / 2
                    
                    # –°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞ –º–µ–∂–¥—É –æ–ø–∏—Å–∞–Ω–∏–µ–º –∏ –ª—É—á—à–∏–º –ø—Ä–∏–º–µ—Ä–æ–º
                    # –° –Ω–µ–±–æ–ª—å—à–∏–º –±–æ–Ω—É—Å–æ–º –∑–∞ –ø—Ä–∏–º–µ—Ä—ã (–æ–Ω–∏ –±–æ–ª–µ–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ)
                    scores[idx] = 0.4 * scores[idx] + 0.6 * max_example_sim
        
        return scores
    
    def _calculate_keyword_scores(self, text: str) -> np.ndarray:
        """
        –†–∞—Å—á–µ—Ç –æ—Ü–µ–Ω–æ–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤.
        –£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏ –∏ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–º–∏ —Å–ª–æ–≤–∞–º–∏.
        
        Args:
            text: —Ç–µ–∫—Å—Ç –∂–∞–ª–æ–±—ã
            
        Returns:
            –ú–∞—Å—Å–∏–≤ –æ—Ü–µ–Ω–æ–∫ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä—É–±—Ä–∏–∫–∞—Ç–æ—Ä–∞
        """
        scores = np.zeros(len(self.rubrics))
        
        for i, rubric in enumerate(self.rubrics):
            score, _ = calculate_advanced_keyword_score(
                text,
                rubric['keywords'],
                rubric.get('priority_keywords', []),
                rubric.get('negative_keywords', [])
            )
            scores[i] = score
        
        return scores
    
    def _apply_rule_based_adjustments(self, text: str, scores: np.ndarray) -> np.ndarray:
        """
        –ü—Ä–∏–º–µ–Ω—è–µ—Ç –ø—Ä–∞–≤–∏–ª–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤.
        –£—Å–∏–ª–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –ø—Ä–∞–≤–∏–ª–∞–º–∏.
        
        Args:
            text: —Ç–µ–∫—Å—Ç –∂–∞–ª–æ–±—ã
            scores: —Ç–µ–∫—É—â–∏–µ –æ—Ü–µ–Ω–∫–∏
            
        Returns:
            –°–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏
        """
        text_lower = text.lower()
        adjusted_scores = scores.copy()
        
        # ============================================================
        # –ë–õ–û–ö 1: –ó–∞–∫–æ–Ω—ã –∏ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –∞–∫—Ç—ã
        # ============================================================
        
        # –ü—Ä–∞–≤–∏–ª–æ 1: 161-–§–ó - –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –∫–∞—Ä—Ç—ã/—Å—á–µ—Ç–∞ (ID=2)
        if '161-—Ñ–∑' in text_lower or '161 —Ñ–∑' in text_lower or '161—Ñ–∑' in text_lower:
            adjusted_scores[1] += 0.35  # –ë–æ–Ω—É—Å –¥–ª—è ID=2
            adjusted_scores[0] -= 0.15  # –®—Ç—Ä–∞—Ñ –¥–ª—è ID=1 (–æ–±—â–∞—è –∂–∞–ª–æ–±–∞ –Ω–∞ –±–∞–Ω–∫)
            adjusted_scores[2] -= 0.1   # –®—Ç—Ä–∞—Ñ –¥–ª—è ID=3 (115-–§–ó)
        
        # –ü—Ä–∞–≤–∏–ª–æ 2: 115-–§–ó –±–µ–∑ —É–ø—Ä–∞–≤–ª—è—é—â–µ–≥–æ/–∞–¥–≤–æ–∫–∞—Ç–∞ - –Ω–∞—Ä—É—à–µ–Ω–∏–µ 115-–§–ó (ID=3)
        if '115-—Ñ–∑' in text_lower or '115 —Ñ–∑' in text_lower or '115—Ñ–∑' in text_lower:
            if '—É–ø—Ä–∞–≤–ª—è—é—â' not in text_lower and '–∞–¥–≤–æ–∫–∞—Ç' not in text_lower:
                adjusted_scores[2] += 0.3  # –ë–æ–Ω—É—Å –¥–ª—è ID=3
                adjusted_scores[1] -= 0.1  # –®—Ç—Ä–∞—Ñ –¥–ª—è ID=2 (161-–§–ó)
        
        # –ü—Ä–∞–≤–∏–ª–æ 2.1: 230-–§–ó - –∫–æ–ª–ª–µ–∫—Ç–æ—Ä—ã (ID=5)
        if '230-—Ñ–∑' in text_lower or '230 —Ñ–∑' in text_lower:
            adjusted_scores[4] += 0.3  # –ë–æ–Ω—É—Å –¥–ª—è ID=5
        
        # –ü—Ä–∞–≤–∏–ª–æ 2.2: 63-–§–ó - –∞–¥–≤–æ–∫–∞—Ç (ID=9)
        if '63-—Ñ–∑' in text_lower or '63 —Ñ–∑' in text_lower:
            adjusted_scores[8] += 0.35  # –ë–æ–Ω—É—Å –¥–ª—è ID=9
        
        # ============================================================
        # –ë–õ–û–ö 2: –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –∏ –æ—Ä–≥–∞–Ω—ã
        # ============================================================
        
        # –ü—Ä–∞–≤–∏–ª–æ 3: –ö–æ–ª–ª–µ–∫—Ç–æ—Ä—ã - —è–≤–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫ (ID=5)
        if '–∫–æ–ª–ª–µ–∫—Ç–æ—Ä' in text_lower or '–≤–∑—ã—Å–∫–∞—Ç–µ–ª—å' in text_lower:
            adjusted_scores[4] += 0.35  # –ë–æ–Ω—É—Å –¥–ª—è ID=5
            adjusted_scores[3] -= 0.2   # –®—Ç—Ä–∞—Ñ –¥–ª—è –§–°–°–ü
            adjusted_scores[0] -= 0.1   # –®—Ç—Ä–∞—Ñ –¥–ª—è –æ–±—â–µ–π –∂–∞–ª–æ–±—ã –Ω–∞ –±–∞–Ω–∫
        
        # –ü—Ä–∞–≤–∏–ª–æ 4: –§–°–°–ü/–ø—Ä–∏—Å—Ç–∞–≤ - —è–≤–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫ (ID=4)
        if '—Ñ—Å—Å–ø' in text_lower or '–ø—Ä–∏—Å—Ç–∞–≤' in text_lower or '–∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω' in text_lower:
            if '–∫–æ–ª–ª–µ–∫—Ç–æ—Ä' not in text_lower:
                adjusted_scores[3] += 0.3  # –ë–æ–Ω—É—Å –¥–ª—è ID=4
                adjusted_scores[4] -= 0.15  # –®—Ç—Ä–∞—Ñ –¥–ª—è –∫–æ–ª–ª–µ–∫—Ç–æ—Ä–æ–≤
        
        # –ü—Ä–∞–≤–∏–ª–æ 4.1: –§–ù–°/–Ω–∞–ª–æ–≥–æ–≤–∞—è (ID=7)
        if '—Ñ–Ω—Å' in text_lower or '–Ω–∞–ª–æ–≥–æ–≤' in text_lower:
            if '—É–∫–ª–æ–Ω' in text_lower or '–Ω–µ—É–ø–ª–∞—Ç' in text_lower or '—Å–µ—Ä–∞—è' in text_lower or '–∫–æ–Ω–≤–µ—Ä—Ç' in text_lower:
                adjusted_scores[6] += 0.35  # –ë–æ–Ω—É—Å –¥–ª—è ID=7
        
        # –ü—Ä–∞–≤–∏–ª–æ 4.2: –§–ê–°/–∞–Ω—Ç–∏–º–æ–Ω–æ–ø–æ–ª—å–Ω—ã–π (ID=6)
        if '—Ñ–∞—Å' in text_lower or '–∞–Ω—Ç–∏–º–æ–Ω–æ–ø–æ–ª—å–Ω' in text_lower or '–º–æ–Ω–æ–ø–æ–ª–∏' in text_lower:
            adjusted_scores[5] += 0.35  # –ë–æ–Ω—É—Å –¥–ª—è ID=6
        
        # ============================================================
        # –ë–õ–û–ö 3: –ú–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ –æ—Ç –∏–º–µ–Ω–∏ –†–æ—Å—Ñ–∏–Ω–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
        # ============================================================
        
        if '—Ä–æ—Å—Ñ–∏–Ω–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥' in text_lower or '—Ä—Ñ–º' in text_lower:
            # ID=12: –ó–≤–æ–Ω–æ–∫ –æ—Ç –º–æ—à–µ–Ω–Ω–∏–∫–æ–≤
            if any(w in text_lower for w in ['–∑–≤–æ–Ω', '–ø–æ–∑–≤–æ–Ω–∏–ª', '–º–æ—à–µ–Ω–Ω–∏–∫', '–±–µ–∑–æ–ø–∞—Å–Ω', '–ø–µ—Ä–µ–≤–µ—Å—Ç–∏']):
                adjusted_scores[11] += 0.4  # ID=12
                # –®—Ç—Ä–∞—Ñ—ã –¥–ª—è –ø–æ—Ö–æ–∂–∏—Ö —Ä—É–±—Ä–∏–∫
                adjusted_scores[12] -= 0.1
                adjusted_scores[13] -= 0.1
                adjusted_scores[14] -= 0.1
            
            # ID=13: –ü–æ–¥–¥–µ–ª—å–Ω–∞—è –¥–æ–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –Ω–∞ –ì–æ—Å—É—Å–ª—É–≥–∞—Ö
            if '–¥–æ–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å' in text_lower:
                adjusted_scores[12] += 0.4  # ID=13
                adjusted_scores[11] -= 0.1
            
            # ID=14: –§–∏—à–∏–Ω–≥–æ–≤–æ–µ –ø–∏—Å—å–º–æ –æ–± –æ–ø–ª–∞—Ç–µ
            if '–ø–∏—Å—å–º–æ' in text_lower and any(w in text_lower for w in ['–æ–ø–ª–∞—Ç', '—à—Ç—Ä–∞—Ñ', '–ø–µ–Ω–∏', '–∫–æ–º–∏—Å—Å–∏', '–ª–∏—Ü–µ–Ω–∑–∏']):
                adjusted_scores[13] += 0.4  # ID=14
                adjusted_scores[11] -= 0.1
            
            # ID=15: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞
            if any(w in text_lower for w in ['–ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç—å', '—è–≤–ª—è–µ—Ç—Å—è –ª–∏', '–ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–æ—Ç—Ä—É–¥–Ω–∏–∫', '—Ä–∞–±–æ—Ç–∞–µ—Ç –ª–∏']):
                adjusted_scores[14] += 0.4  # ID=15
                adjusted_scores[11] -= 0.15
        
        # ============================================================
        # –ë–õ–û–ö 4: –ì–æ—Å—É—Å–ª—É–≥–∏ –∏ –≤–∑–ª–æ–º
        # ============================================================
        
        if '–≥–æ—Å—É—Å–ª—É–≥' in text_lower or '–µ–ø–≥—É' in text_lower or 'esia' in text_lower:
            # ID=17 vs ID=18: –≤–∑–ª–æ–º —Å –ø–æ–ª–∏—Ü–∏–µ–π –∏–ª–∏ –±–µ–∑
            if any(w in text_lower for w in ['–≤–∑–ª–æ–º', '–≤–∑–ª–æ–º–∞–ª', '–≤–∑–ª–æ–º–∞–ª–∏', '–Ω–µ—Å–∞–Ω–∫—Ü–∏–æ–Ω–∏—Ä–æ–≤', '—É–∫—Ä–∞–ª–∏ –¥–∞–Ω–Ω—ã–µ']):
                if any(w in text_lower for w in ['–ø–æ–ª–∏—Ü–∏', '–º–≤–¥', '–∑–∞—è–≤–ª–µ–Ω–∏', '—É–≥–æ–ª–æ–≤–Ω', '–≤–æ–∑–±—É–∂–¥']):
                    adjusted_scores[17] += 0.35  # ID=18 - –æ–±—Ä–∞—Ç–∏–ª—Å—è –≤ –ø–æ–ª–∏—Ü–∏—é
                    adjusted_scores[16] -= 0.2   # –®—Ç—Ä–∞—Ñ –¥–ª—è ID=17
                else:
                    adjusted_scores[16] += 0.35  # ID=17 - –ø—Ä–æ—Å—Ç–æ –≤–∑–ª–æ–º
                    adjusted_scores[17] -= 0.15  # –®—Ç—Ä–∞—Ñ –¥–ª—è ID=18
            
            # ID=13: –î–æ–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –Ω–∞ –ì–æ—Å—É—Å–ª—É–≥–∞—Ö
            if '–¥–æ–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å' in text_lower:
                adjusted_scores[12] += 0.3  # ID=13
        
        # ============================================================
        # –ë–õ–û–ö 5: –ö—Ä–µ–¥–∏—Ç—ã –∏ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ
        # ============================================================
        
        if '–∫—Ä–µ–¥–∏—Ç' in text_lower or '–∑–∞–π–º' in text_lower or '–∑–∞–π–º–∞' in text_lower:
            # ID=16: –ö—Ä–µ–¥–∏—Ç –æ—Ñ–æ—Ä–º–ª–µ–Ω –º–æ—à–µ–Ω–Ω–∏–∫–∞–º–∏
            if any(phrase in text_lower for phrase in ['–Ω–µ –±—Ä–∞–ª', '–Ω–µ –æ—Ñ–æ—Ä–º–ª—è–ª', '–±–µ–∑ —Å–æ–≥–ª–∞—Å–∏—è', '–±–µ–∑ –≤–µ–¥–æ–º–∞', '–º–æ—à–µ–Ω–Ω–∏–∫', '–Ω–µ –ø–æ–¥–ø–∏—Å—ã–≤–∞–ª']):
                adjusted_scores[15] += 0.4  # ID=16
                adjusted_scores[0] -= 0.15   # –®—Ç—Ä–∞—Ñ –¥–ª—è –æ–±—â–µ–π –∂–∞–ª–æ–±—ã –Ω–∞ –±–∞–Ω–∫
            # ID=1: –û–±—ã—á–Ω–∞—è –∂–∞–ª–æ–±–∞ –Ω–∞ –∫—Ä–µ–¥–∏—Ç
            elif any(phrase in text_lower for phrase in ['–Ω–∞–≤—è–∑–∞–ª', '—Å—Ç—Ä–∞—Ö–æ–≤–∫', '–ø—Ä–æ—Ü–µ–Ω—Ç', '—É—Å–ª–æ–≤–∏—è']):
                adjusted_scores[0] += 0.2  # ID=1
        
        # ============================================================
        # –ë–õ–û–ö 6: –°–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ —Ç–µ–º—ã
        # ============================================================
        
        # –ü—Ä–∞–≤–∏–ª–æ: –ú–µ–∂–≤–µ–¥–æ–º—Å—Ç–≤–µ–Ω–Ω–∞—è –∫–æ–º–∏—Å—Å–∏—è (ID=19)
        if '–º–µ–∂–≤–µ–¥–æ–º—Å—Ç–≤–µ–Ω–Ω' in text_lower and '–∫–æ–º–∏—Å—Å–∏' in text_lower:
            adjusted_scores[18] += 0.5  # ID=19 - –æ—á–µ–Ω—å —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–∞—è —Ç–µ–º–∞
        
        # –ü—Ä–∞–≤–∏–ª–æ: –û—à–∏–±–æ—á–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ (ID=20)
        if any(phrase in text_lower for phrase in ['–æ—à–∏–±–æ—á–Ω', '—Å–ª—É—á–∞–π–Ω–æ –ø–µ—Ä–µ–≤–µ–ª', '–ø–µ—Ä–µ–ø—É—Ç–∞–ª']):
            if any(w in text_lower for w in ['–ø–µ—Ä–µ–≤–æ–¥', '–ø–µ—Ä–µ–≤–µ–ª', '–æ—Ç–ø—Ä–∞–≤–∏–ª', '—Ä–µ–∫–≤–∏–∑–∏—Ç', '–Ω–æ–º–µ—Ä']):
                adjusted_scores[19] += 0.4  # ID=20
        if '—Å–±–ø' in text_lower and any(w in text_lower for w in ['–æ—à–∏–±', '–Ω–µ —Ç–æ–º—É', '–Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω']):
            adjusted_scores[19] += 0.3  # ID=20
        
        # –ü—Ä–∞–≤–∏–ª–æ: –ö–∞–∑–∏–Ω–æ (ID=10 vs ID=11)
        if '–∫–∞–∑–∏–Ω–æ' in text_lower or '–∞–∑–∞—Ä—Ç–Ω' in text_lower or '—Å—Ç–∞–≤–∫' in text_lower:
            # ID=10: –†–µ–∫–≤–∏–∑–∏—Ç—ã –∫–∞–∑–∏–Ω–æ (–∏–Ω—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ)
            if any(w in text_lower for w in ['—Ä–µ–∫–≤–∏–∑–∏—Ç', '–ø–æ–ø–æ–ª–Ω–µ–Ω–∏', '–∫–∞—Ä—Ç–∞ –¥–ª—è', '—Å–æ–æ–±—â–∞—é']):
                adjusted_scores[9] += 0.35  # ID=10
                adjusted_scores[10] -= 0.15  # –®—Ç—Ä–∞—Ñ –¥–ª—è ID=11
            # ID=11: –ñ–∞–ª–æ–±–∞ –Ω–∞ –∫–∞–∑–∏–Ω–æ (–Ω–µ –≤—ã–ø–ª–∞—á–∏–≤–∞—é—Ç)
            elif any(w in text_lower for w in ['–≤—ã–ø–ª–∞—Ç', '–≤—ã–≤–æ–¥', '–Ω–µ –≤—ã–ø–ª–∞—á', '–∑–∞–±–ª–æ–∫–∏—Ä–æ–≤', '–æ–±–º–∞–Ω—É–ª–∏']):
                adjusted_scores[10] += 0.35  # ID=11
                adjusted_scores[9] -= 0.15   # –®—Ç—Ä–∞—Ñ –¥–ª—è ID=10
        
        # –ü—Ä–∞–≤–∏–ª–æ: –§–∏–Ω–∞–Ω—Å–æ–≤—ã–π/–∫–æ–Ω–∫—É—Ä—Å–Ω—ã–π —É–ø—Ä–∞–≤–ª—è—é—â–∏–π (ID=8)
        if '—É–ø—Ä–∞–≤–ª—è—é—â' in text_lower:
            if any(w in text_lower for w in ['—Ñ–∏–Ω–∞–Ω—Å–æ–≤', '–∫–æ–Ω–∫—É—Ä—Å–Ω', '–∞—Ä–±–∏—Ç—Ä–∞–∂–Ω', '–±–∞–Ω–∫—Ä–æ—Ç—Å—Ç–≤']):
                adjusted_scores[7] += 0.4  # ID=8
                adjusted_scores[8] -= 0.15  # –®—Ç—Ä–∞—Ñ –¥–ª—è –∞–¥–≤–æ–∫–∞—Ç–∞
        
        # –ü—Ä–∞–≤–∏–ª–æ: –ê–¥–≤–æ–∫–∞—Ç (ID=9)
        if '–∞–¥–≤–æ–∫–∞—Ç' in text_lower:
            if any(w in text_lower for w in ['–∑–∞–ø—Ä–æ—Å', '63-—Ñ–∑', '–¥–æ–≤–µ—Ä–∏—Ç–µ–ª', '—é—Ä–∏–¥–∏—á–µ—Å–∫']):
                adjusted_scores[8] += 0.4  # ID=9
                adjusted_scores[7] -= 0.15  # –®—Ç—Ä–∞—Ñ –¥–ª—è —É–ø—Ä–∞–≤–ª—è—é—â–µ–≥–æ
        
        # ============================================================
        # –ë–õ–û–ö 7: –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ (–∏—Å–∫–ª—é—á–µ–Ω–∏—è)
        # ============================================================
        
        # –ï—Å–ª–∏ —è–≤–Ω–æ —É–ø–æ–º–∏–Ω–∞—é—Ç—Å—è –º–æ—à–µ–Ω–Ω–∏–∫–∏ - —à—Ç—Ä–∞—Ñ –¥–ª—è –æ–±—ã—á–Ω—ã—Ö –∂–∞–ª–æ–±
        if '–º–æ—à–µ–Ω–Ω–∏–∫' in text_lower and '—Ä–æ—Å—Ñ–∏–Ω–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥' not in text_lower:
            adjusted_scores[0] -= 0.1  # ID=1
            adjusted_scores[1] -= 0.1  # ID=2
            adjusted_scores[2] -= 0.1  # ID=3
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è
        adjusted_scores = np.clip(adjusted_scores, 0, 1)
        
        return adjusted_scores
    
    def _apply_mutual_exclusion(self, scores: np.ndarray) -> np.ndarray:
        """
        –ü—Ä–∏–º–µ–Ω—è–µ—Ç –≤–∑–∞–∏–º–æ–∏—Å–∫–ª—é—á–∞—é—â–∏–µ –ø—Ä–∞–≤–∏–ª–∞ –º–µ–∂–¥—É —Ä—É–±—Ä–∏–∫–∞–º–∏.
        –ï—Å–ª–∏ –æ–¥–Ω–∞ —Ä—É–±—Ä–∏–∫–∞ –∏–∑ –≥—Ä—É–ø–ø—ã –∏–º–µ–µ—Ç –≤—ã—Å–æ–∫–∏–π score, –¥—Ä—É–≥–∏–µ –ø–æ–ª—É—á–∞—é—Ç —à—Ç—Ä–∞—Ñ.
        
        Args:
            scores: —Ç–µ–∫—É—â–∏–µ –æ—Ü–µ–Ω–∫–∏
            
        Returns:
            –°–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏
        """
        adjusted_scores = scores.copy()
        
        for group in MUTUALLY_EXCLUSIVE_GROUPS:
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã (ID - 1)
            indices = [rubric_id - 1 for rubric_id in group]
            
            # –ù–∞—Ö–æ–¥–∏–º –ª–∏–¥–µ—Ä–∞ –≤ –≥—Ä—É–ø–ø–µ
            group_scores = [(idx, adjusted_scores[idx]) for idx in indices]
            group_scores.sort(key=lambda x: x[1], reverse=True)
            
            if len(group_scores) >= 2:
                leader_idx, leader_score = group_scores[0]
                second_idx, second_score = group_scores[1]
                
                # –ï—Å–ª–∏ –ª–∏–¥–µ—Ä –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –≤–ø–µ—Ä–µ–¥–∏, —à—Ç—Ä–∞—Ñ—É–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã—Ö
                if leader_score > 0.5 and leader_score - second_score > 0.1:
                    for idx, score in group_scores[1:]:
                        # –®—Ç—Ä–∞—Ñ –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª–µ–Ω —Ä–∞–∑–Ω–∏—Ü–µ
                        penalty = (leader_score - score) * 0.3
                        adjusted_scores[idx] = max(0, adjusted_scores[idx] - penalty)
        
        return adjusted_scores
    
    def _filter_predictions(self, predictions: List[Dict], best_confidence: float) -> List[Dict]:
        """
        –§–∏–ª—å—Ç—Ä—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –ª–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π.
        
        Args:
            predictions: —Å–ø–∏—Å–æ–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
            best_confidence: confidence –ª—É—á—à–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            
        Returns:
            –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫
        """
        if len(predictions) <= 1:
            return predictions
        
        filtered = [predictions[0]]  # –õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤—Å–µ–≥–¥–∞ –≤–∫–ª—é—á–∞–µ–º
        
        for pred in predictions[1:]:
            confidence = pred['confidence']
            gap = best_confidence - confidence
            
            # –£—Å–ª–æ–≤–∏—è –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –≤–∞—Ä–∏–∞–Ω—Ç–∞:
            # 1. Confidence –≤—ã—à–µ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞
            # 2. –†–∞–∑—Ä—ã–≤ —Å –ª–∏–¥–µ—Ä–æ–º –Ω–µ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π
            # 3. –ï—Å–ª–∏ –ª–∏–¥–µ—Ä –æ—á–µ–Ω—å —É–≤–µ—Ä–µ–Ω (>0.7), —Ç—Ä–µ–±—É–µ–º –º–µ–Ω—å—à–∏–π gap
            
            if confidence >= MIN_CONFIDENCE_THRESHOLD:
                if best_confidence > 0.7:
                    # –õ–∏–¥–µ—Ä —É–≤–µ—Ä–µ–Ω - —Ç—Ä–µ–±—É–µ–º gap < 0.1
                    if gap < 0.1:
                        filtered.append(pred)
                elif best_confidence > 0.5:
                    # –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å - —Ç—Ä–µ–±—É–µ–º gap < MIN_GAP_THRESHOLD
                    if gap < MIN_GAP_THRESHOLD:
                        filtered.append(pred)
                else:
                    # –ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å - –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –±–æ–ª—å—à–µ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤
                    if gap < MIN_GAP_THRESHOLD * 1.5:
                        filtered.append(pred)
        
        return filtered
    
    def predict(
        self,
        text: str,
        top_k: int = 3,
        return_scores: bool = True
    ) -> Dict:
        """
        –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∂–∞–ª–æ–±—ã.
        
        Args:
            text: —Ç–µ–∫—Å—Ç –∂–∞–ª–æ–±—ã
            top_k: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            return_scores: –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å –ª–∏ –¥–µ—Ç–∞–ª—å–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        """
        if self.model is None or self.rubric_embeddings is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ load() –∏–ª–∏ train()")
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–µ–∫—Å—Ç
        text_normalized = normalize_text(text)
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –æ—Ü–µ–Ω–∫–∏
        semantic_scores = self._calculate_semantic_scores(text)
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –æ—Ü–µ–Ω–∫–∏ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        if self.use_keywords:
            keyword_scores = self._calculate_keyword_scores(text)
            # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –æ—Ü–µ–Ω–∫–∏
            combined_scores = (
                self.semantic_weight * semantic_scores +
                self.keyword_weight * keyword_scores
            )
        else:
            keyword_scores = np.zeros(len(self.rubrics))
            combined_scores = semantic_scores
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–∞–≤–∏–ª–∞
        rule_adjusted_scores = self._apply_rule_based_adjustments(text, combined_scores)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –≤–∑–∞–∏–º–æ–∏—Å–∫–ª—é—á–∞—é—â–∏–µ –ø—Ä–∞–≤–∏–ª–∞
        final_scores = self._apply_mutual_exclusion(rule_adjusted_scores)
        
        # –ù–∞—Ö–æ–¥–∏–º —Ç–æ–ø-k —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        top_indices = np.argsort(final_scores)[::-1][:top_k]
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        predictions = []
        for idx in top_indices:
            rubric = self.rubrics[idx]
            rubric_id = rubric['id']
            predictions.append({
                'rubric_id': rubric_id,
                'rubric_name': rubric['description'],  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
                'short_name': rubric['name'],  # –ö—Ä–∞—Ç–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–ª—è —Å–ø—Ä–∞–≤–∫–∏
                'response_template': get_response_template(rubric_id),  # –®–∞–±–ª–æ–Ω –æ—Ç–≤–µ—Ç–∞
                'confidence': float(final_scores[idx]),
                'semantic_score': float(semantic_scores[idx]) if return_scores else None,
                'keyword_score': float(keyword_scores[idx]) if return_scores else None
            })
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –ª–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π
        if len(predictions) > 1:
            predictions = self._filter_predictions(predictions, predictions[0]['confidence'])
        
        result = {
            'text': text,
            'predictions': predictions,
            'best_match': predictions[0] if predictions else None
        }
        
        return result
    
    def predict_batch(
        self,
        texts: List[str],
        top_k: int = 1
    ) -> List[Dict]:
        """
        –ü–∞–∫–µ—Ç–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∂–∞–ª–æ–± —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π.
        
        Args:
            texts: —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –∂–∞–ª–æ–±
            top_k: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        """
        if self.model is None or self.rubric_embeddings is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ load() –∏–ª–∏ train()")
        
        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö —Ç–µ–∫—Å—Ç–æ–≤ (–±–∞—Ç—á)
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        embeddings = []
        texts_to_encode = []
        text_indices = []
        
        for i, text in enumerate(texts):
            cached = self.embedding_cache.get(text)
            if cached is not None:
                embeddings.append((i, cached))
            else:
                texts_to_encode.append(text)
                text_indices.append(i)
        
        # –ö–æ–¥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ –∫—ç—à–µ
        if texts_to_encode:
            new_embeddings = self.model.encode(texts_to_encode, convert_to_numpy=True)
            for i, (text_idx, emb) in enumerate(zip(text_indices, new_embeddings)):
                embeddings.append((text_idx, emb))
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
                self.embedding_cache.set(texts_to_encode[i], emb)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∏—Å—Ö–æ–¥–Ω–æ–º—É –∏–Ω–¥–µ–∫—Å—É
        embeddings.sort(key=lambda x: x[0])
        embeddings = [emb for _, emb in embeddings]
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π —Ç–µ–∫—Å—Ç
        results = []
        for i, text in enumerate(texts):
            result = self.predict(text, top_k=top_k, return_scores=False)
            results.append(result)
        
        return results
    
    def get_cache_stats(self) -> Dict:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫—ç—à–∞"""
        return self.embedding_cache.stats()
    
    def clear_cache(self):
        """–û—á–∏—Å—Ç–∏—Ç—å –∫—ç—à —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        self.embedding_cache.clear()
