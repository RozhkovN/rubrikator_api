"""
–û—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –∂–∞–ª–æ–±.

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥:
1. Sentence Transformers –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
2. –ê–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è —É—Ç–æ—á–Ω–µ–Ω–∏—è
3. –ü—Ä–∏–º–µ—Ä—ã –∂–∞–ª–æ–± –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
4. –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –∏ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
5. –í–∑–∞–∏–º–æ–∏—Å–∫–ª—é—á–∞—é—â–∏–µ –ø—Ä–∞–≤–∏–ª–∞ –º–µ–∂–¥—É —Ä—É–±—Ä–∏–∫–∞–º–∏
6. LRU –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
7. –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤
"""

import os
import pickle
import hashlib
import numpy as np
from typing import List, Dict, Tuple, Optional
from functools import lru_cache
from collections import OrderedDict
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

from config.rubrics import RUBRICS, get_rubric_by_id
from config.response_templates import get_response_template
from src.preprocessor import (
    normalize_text, 
    calculate_keyword_score,
    calculate_advanced_keyword_score,
    extract_law_references,
    extract_organization_mentions
)


# ===================== –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –§–ò–õ–¨–¢–†–ê–¶–ò–ò =====================

# –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ confidence –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤
MIN_CONFIDENCE_THRESHOLD = 0.25

# –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑—Ä—ã–≤ –º–µ–∂–¥—É –ª—É—á—à–∏–º –∏ –≤—Ç–æ—Ä—ã–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º (–µ—Å–ª–∏ –±–æ–ª—å—à–µ - –Ω–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –≤—Ç–æ—Ä–æ–π)
MAX_GAP_THRESHOLD = 0.35

# –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π confidence –¥–ª—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞, —á—Ç–æ–±—ã —Å—á–∏—Ç–∞—Ç—å –µ–≥–æ —É–≤–µ—Ä–µ–Ω–Ω—ã–º
HIGH_CONFIDENCE_THRESHOLD = 0.65


# ===================== –í–ó–ê–ò–ú–û–ò–°–ö–õ–Æ–ß–ê–Æ–©–ò–ï –ü–†–ê–í–ò–õ–ê =====================
# –ï—Å–ª–∏ –≤—ã–±—Ä–∞–Ω–∞ –æ–¥–Ω–∞ —Ä—É–±—Ä–∏–∫–∞, –¥—Ä—É–≥–∏–µ –∏–∑ —ç—Ç–æ–≥–æ —Å–ø–∏—Å–∫–∞ –ø–æ–ª—É—á–∞—é—Ç —à—Ç—Ä–∞—Ñ
MUTUALLY_EXCLUSIVE_GROUPS = [
    # –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∏: 161-–§–ó vs 115-–§–ó
    {2, 3},
    
    # –§–°–°–ü vs –ö–æ–ª–ª–µ–∫—Ç–æ—Ä—ã
    {4, 5},
    
    # –ö–∞–∑–∏–Ω–æ: —Ä–µ–∫–≤–∏–∑–∏—Ç—ã vs –∂–∞–ª–æ–±–∞
    {10, 11},
    
    # –ú–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ –æ—Ç –∏–º–µ–Ω–∏ –†–§–ú: —Ä–∞–∑–Ω—ã–µ —Ç–∏–ø—ã
    {12, 13, 14, 15},
    
    # –ì–æ—Å—É—Å–ª—É–≥–∏: –≤–∑–ª–æ–º vs –≤–∑–ª–æ–º+–ø–æ–ª–∏—Ü–∏—è
    {17, 18},
    
    # –£–ø—Ä–∞–≤–ª—è—é—â–∏–π vs –ê–¥–≤–æ–∫–∞—Ç
    {8, 9},
]


class LRUCache:
    """LRU –∫—ç—à –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ç–µ–∫—Å—Ç–æ–≤"""
    
    def __init__(self, max_size: int = 1000):
        self.cache = OrderedDict()
        self.max_size = max_size
        self.hits = 0
        self.misses = 0
    
    def _hash_text(self, text: str) -> str:
        """–°–æ–∑–¥–∞—ë—Ç —Ö—ç—à —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∫–ª—é—á–∞ –∫—ç—à–∞"""
        return hashlib.md5(text.encode('utf-8')).hexdigest()
    
    def get(self, text: str) -> Optional[np.ndarray]:
        """–ü–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥ –∏–∑ –∫—ç—à–∞"""
        key = self._hash_text(text)
        if key in self.cache:
            self.hits += 1
            # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –≤ –∫–æ–Ω–µ—Ü (–Ω–µ–¥–∞–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–π)
            self.cache.move_to_end(key)
            return self.cache[key]
        self.misses += 1
        return None
    
    def put(self, text: str, embedding: np.ndarray):
        """–î–æ–±–∞–≤–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥ –≤ –∫—ç—à"""
        key = self._hash_text(text)
        if key in self.cache:
            self.cache.move_to_end(key)
        else:
            if len(self.cache) >= self.max_size:
                # –£–¥–∞–ª—è–µ–º —Å–∞–º—ã–π —Å—Ç–∞—Ä—ã–π —ç–ª–µ–º–µ–Ω—Ç
                self.cache.popitem(last=False)
            self.cache[key] = embedding
    
    def get_stats(self) -> Dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—ç—à–∞"""
        total = self.hits + self.misses
        hit_rate = self.hits / total if total > 0 else 0
        return {
            "size": len(self.cache),
            "max_size": self.max_size,
            "hits": self.hits,
            "misses": self.misses,
            "hit_rate": f"{hit_rate:.2%}"
        }
    
    def clear(self):
        """–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞"""
        self.cache.clear()
        self.hits = 0
        self.misses = 0


class ComplaintClassifier:
    """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∂–∞–ª–æ–± –Ω–∞ –æ—Å–Ω–æ–≤–µ Sentence Transformers"""
    
    def __init__(
        self,
        model_name: str = "paraphrase-multilingual-mpnet-base-v2",
        use_keywords: bool = True,
        keyword_weight: float = 0.35,
        use_examples: bool = True,
        cache_size: int = 1000,
        use_onnx: bool = False
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞.
        
        Args:
            model_name: –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ Sentence Transformers
            use_keywords: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ –∞–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            keyword_weight: –≤–µ—Å –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ (0-1), –æ—Å—Ç–∞–ª—å–Ω–æ–µ - —Å–µ–º–∞–Ω—Ç–∏–∫–∞
            use_examples: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            cache_size: —Ä–∞–∑–º–µ—Ä LRU –∫—ç—à–∞ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            use_onnx: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ ONNX Runtime –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        """
        self.model_name = model_name
        self.use_keywords = use_keywords
        self.keyword_weight = keyword_weight
        self.semantic_weight = 1 - keyword_weight
        self.use_examples = use_examples
        self.use_onnx = use_onnx
        
        self.model: Optional[SentenceTransformer] = None
        self.rubric_embeddings: Optional[np.ndarray] = None
        self.example_embeddings: Optional[Dict[int, np.ndarray]] = None
        self.rubrics = RUBRICS
        
        # LRU –∫—ç—à –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        self.embedding_cache = LRUCache(max_size=cache_size)
        
        print(f"üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞...")
        print(f"   –ú–æ–¥–µ–ª—å: {model_name}")
        print(f"   –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {'–î–∞' if use_keywords else '–ù–µ—Ç'}")
        print(f"   –ü—Ä–∏–º–µ—Ä—ã: {'–î–∞' if use_examples else '–ù–µ—Ç'}")
        print(f"   –ö—ç—à: {cache_size} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
        print(f"   ONNX: {'–î–∞' if use_onnx else '–ù–µ—Ç'}")
        if use_keywords:
            print(f"   –í–µ—Å–∞: —Å–µ–º–∞–Ω—Ç–∏–∫–∞={self.semantic_weight:.2f}, –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞={self.keyword_weight:.2f}")
    
    def load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Sentence Transformers —Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π ONNX –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π"""
        if self.model is None:
            print(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {self.model_name}...")
            
            # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å ONNX backend –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
            if self.use_onnx:
                try:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å ONNX Runtime
                    import onnxruntime
                    print("   üöÄ –ò—Å–ø–æ–ª—å–∑—É–µ–º ONNX Runtime –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è...")
                    
                    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å ONNX backend
                    self.model = SentenceTransformer(
                        self.model_name,
                        backend="onnx"
                    )
                    print("‚úì –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —Å ONNX –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π")
                except ImportError:
                    print("   ‚ö†Ô∏è ONNX Runtime –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π backend")
                    self.model = SentenceTransformer(self.model_name)
                    print("‚úì –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ä–µ–∂–∏–º)")
                except Exception as e:
                    print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ ONNX: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π backend")
                    self.model = SentenceTransformer(self.model_name)
                    print("‚úì –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ä–µ–∂–∏–º)")
            else:
                self.model = SentenceTransformer(self.model_name)
                print("‚úì –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    
    def prepare_rubric_texts(self) -> List[str]:
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π —Ä—É–±—Ä–∏–∫–∞—Ç–æ—Ä–æ–≤.
        –£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏.
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
        """
        texts = []
        for rubric in self.rubrics:
            # –ë–∞–∑–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
            parts = [rubric['description']]
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            parts.append(f"–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {', '.join(rubric['keywords'])}")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            if 'priority_keywords' in rubric and rubric['priority_keywords']:
                parts.append(f"–í–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {', '.join(rubric['priority_keywords'])}")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            if self.use_examples and 'examples' in rubric and rubric['examples']:
                examples_text = " | ".join(rubric['examples'][:3])  # –ë–µ—Ä–µ–º –¥–æ 3 –ø—Ä–∏–º–µ—Ä–æ–≤
                parts.append(f"–ü—Ä–∏–º–µ—Ä—ã –æ–±—Ä–∞—â–µ–Ω–∏–π: {examples_text}")
            
            text = ". ".join(parts)
            texts.append(text)
        
        return texts
    
    def train(self, save_path: str = "models/classifier.pkl"):
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ (—Å–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ä—É–±—Ä–∏–∫–∞—Ç–æ—Ä–æ–≤).
        
        Args:
            save_path: –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        """
        print("\nüéØ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å, –µ—Å–ª–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞
        self.load_model()
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ä—É–±—Ä–∏–∫–∞—Ç–æ—Ä–æ–≤
        print("üìù –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ–ø–∏—Å–∞–Ω–∏–π —Ä—É–±—Ä–∏–∫–∞—Ç–æ—Ä–æ–≤...")
        rubric_texts = self.prepare_rubric_texts()
        
        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ä—É–±—Ä–∏–∫–∞—Ç–æ—Ä–æ–≤
        print("üîÑ –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π —Ä—É–±—Ä–∏–∫–∞—Ç–æ—Ä–æ–≤...")
        self.rubric_embeddings = self.model.encode(
            rubric_texts,
            show_progress_bar=True,
            convert_to_numpy=True
        )
        
        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä—É–±—Ä–∏–∫–∞—Ç–æ—Ä–∞
        self.example_embeddings = {}
        if self.use_examples:
            print("üîÑ –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –ø—Ä–∏–º–µ—Ä–æ–≤...")
            for rubric in self.rubrics:
                if 'examples' in rubric and rubric['examples']:
                    embeddings = self.model.encode(
                        rubric['examples'],
                        convert_to_numpy=True
                    )
                    self.example_embeddings[rubric['id']] = embeddings
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        with open(save_path, 'wb') as f:
            pickle.dump({
                'embeddings': self.rubric_embeddings,
                'example_embeddings': self.example_embeddings,
                'model_name': self.model_name,
                'use_keywords': self.use_keywords,
                'keyword_weight': self.keyword_weight,
                'use_examples': self.use_examples
            }, f)
        
        print(f"‚úì –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {save_path}")
        print(f"‚úì –°–æ–∑–¥–∞–Ω–æ {len(self.rubric_embeddings)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ä—É–±—Ä–∏–∫–∞—Ç–æ—Ä–æ–≤")
        print(f"‚úì –°–æ–∑–¥–∞–Ω–æ {len(self.example_embeddings)} –Ω–∞–±–æ—Ä–æ–≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    def load(self, load_path: str = "models/classifier.pkl"):
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞.
        
        Args:
            load_path: –ø—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        """
        print(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –∏–∑ {load_path}...")
        
        with open(load_path, 'rb') as f:
            data = pickle.load(f)
        
        self.rubric_embeddings = data['embeddings']
        self.example_embeddings = data.get('example_embeddings', {})
        self.model_name = data['model_name']
        self.use_keywords = data.get('use_keywords', True)
        self.keyword_weight = data.get('keyword_weight', 0.35)
        self.use_examples = data.get('use_examples', True)
        self.semantic_weight = 1 - self.keyword_weight
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        self.load_model()
        
        print("‚úì –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω")
    
    def _calculate_semantic_scores(self, text: str) -> np.ndarray:
        """
        –†–∞—Å—á–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Ü–µ–Ω–æ–∫ —á–µ—Ä–µ–∑ cosine similarity.
        –£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å —É—á–µ—Ç–æ–º –ø—Ä–∏–º–µ—Ä–æ–≤ –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º.
        
        Args:
            text: —Ç–µ–∫—Å—Ç –∂–∞–ª–æ–±—ã
            
        Returns:
            –ú–∞—Å—Å–∏–≤ –æ—Ü–µ–Ω–æ–∫ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä—É–±—Ä–∏–∫–∞—Ç–æ—Ä–∞
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        cached_embedding = self.embedding_cache.get(text)
        if cached_embedding is not None:
            text_embedding = cached_embedding
        else:
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ —Ç–µ–∫—Å—Ç–∞
            text_embedding = self.model.encode([text], convert_to_numpy=True)
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
            self.embedding_cache.put(text, text_embedding)
        
        # –°—á–∏—Ç–∞–µ–º cosine similarity —Å –æ–ø–∏—Å–∞–Ω–∏—è–º–∏ —Ä—É–±—Ä–∏–∫–∞—Ç–æ—Ä–æ–≤
        rubric_similarities = cosine_similarity(text_embedding, self.rubric_embeddings)[0]
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤ –¥–∏–∞–ø–∞–∑–æ–Ω [0, 1]
        scores = (rubric_similarities + 1) / 2
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏
        if self.use_examples and self.example_embeddings:
            for rubric in self.rubrics:
                rubric_id = rubric['id']
                idx = rubric_id - 1  # ID –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å 1
                
                if rubric_id in self.example_embeddings:
                    example_emb = self.example_embeddings[rubric_id]
                    example_sim = cosine_similarity(text_embedding, example_emb)[0]
                    
                    # –ë–µ—Ä–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏
                    max_example_sim = (np.max(example_sim) + 1) / 2
                    
                    # –°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞ –º–µ–∂–¥—É –æ–ø–∏—Å–∞–Ω–∏–µ–º –∏ –ª—É—á—à–∏–º –ø—Ä–∏–º–µ—Ä–æ–º
                    # –° –Ω–µ–±–æ–ª—å—à–∏–º –±–æ–Ω—É—Å–æ–º –∑–∞ –ø—Ä–∏–º–µ—Ä—ã (–æ–Ω–∏ –±–æ–ª–µ–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ)
                    scores[idx] = 0.4 * scores[idx] + 0.6 * max_example_sim
        
        return scores
    
    def _calculate_keyword_scores(self, text: str) -> np.ndarray:
        """
        –†–∞—Å—á–µ—Ç –æ—Ü–µ–Ω–æ–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤.
        –£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏ –∏ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–º–∏ —Å–ª–æ–≤–∞–º–∏.
        
        Args:
            text: —Ç–µ–∫—Å—Ç –∂–∞–ª–æ–±—ã
            
        Returns:
            –ú–∞—Å—Å–∏–≤ –æ—Ü–µ–Ω–æ–∫ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä—É–±—Ä–∏–∫–∞—Ç–æ—Ä–∞
        """
        scores = np.zeros(len(self.rubrics))
        
        for i, rubric in enumerate(self.rubrics):
            score, _ = calculate_advanced_keyword_score(
                text,
                rubric['keywords'],
                rubric.get('priority_keywords', []),
                rubric.get('negative_keywords', [])
            )
            scores[i] = score
        
        return scores
    
    def _apply_rule_based_adjustments(self, text: str, scores: np.ndarray) -> np.ndarray:
        """
        –ü—Ä–∏–º–µ–Ω—è–µ—Ç –ø—Ä–∞–≤–∏–ª–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤.
        –£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –ø—Ä–∞–≤–∏–ª–∞–º–∏.
        
        Args:
            text: —Ç–µ–∫—Å—Ç –∂–∞–ª–æ–±—ã
            scores: —Ç–µ–∫—É—â–∏–µ –æ—Ü–µ–Ω–∫–∏
            
        Returns:
            –°–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏
        """
        text_lower = text.lower()
        adjusted_scores = scores.copy()
        
        # ===================== –ü–†–ê–í–ò–õ–ê –î–õ–Ø –ó–ê–ö–û–ù–û–í =====================
        
        # –ü—Ä–∞–≤–∏–ª–æ 1: –ï—Å–ª–∏ –µ—Å—Ç—å 161-–§–ó - —ç—Ç–æ —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ (ID=2)
        if '161-—Ñ–∑' in text_lower or '161 —Ñ–∑' in text_lower or '161—Ñ–∑' in text_lower:
            adjusted_scores[1] += 0.30  # –ë–æ–Ω—É—Å –¥–ª—è ID=2
            adjusted_scores[0] -= 0.15   # –®—Ç—Ä–∞—Ñ –¥–ª—è ID=1
            adjusted_scores[2] -= 0.10   # –®—Ç—Ä–∞—Ñ –¥–ª—è ID=3 (115-–§–ó)
        
        # –ü—Ä–∞–≤–∏–ª–æ 2: –ï—Å–ª–∏ –µ—Å—Ç—å 115-–§–ó –±–µ–∑ —É–ø—Ä–∞–≤–ª—è—é—â–µ–≥–æ/–∞–¥–≤–æ–∫–∞—Ç–∞ - —ç—Ç–æ ID=3
        if ('115-—Ñ–∑' in text_lower or '115 —Ñ–∑' in text_lower or '115—Ñ–∑' in text_lower):
            if '—É–ø—Ä–∞–≤–ª—è—é—â' not in text_lower and '–∞–¥–≤–æ–∫–∞—Ç' not in text_lower:
                adjusted_scores[2] += 0.25  # –ë–æ–Ω—É—Å –¥–ª—è ID=3
                adjusted_scores[1] -= 0.10  # –®—Ç—Ä–∞—Ñ –¥–ª—è ID=2 (161-–§–ó)
        
        # –ü—Ä–∞–≤–∏–ª–æ 2.1: 230-–§–ó - –∫–æ–ª–ª–µ–∫—Ç–æ—Ä—ã
        if '230-—Ñ–∑' in text_lower or '230 —Ñ–∑' in text_lower:
            adjusted_scores[4] += 0.25  # –ë–æ–Ω—É—Å –¥–ª—è ID=5
        
        # ===================== –ü–†–ê–í–ò–õ–ê –î–õ–Ø –í–ó–´–°–ö–ê–ù–ò–Ø =====================
        
        # –ü—Ä–∞–≤–∏–ª–æ 3: –ö–æ–ª–ª–µ–∫—Ç–æ—Ä—ã - —è–≤–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫
        if '–∫–æ–ª–ª–µ–∫—Ç–æ—Ä' in text_lower or '–∫–æ–ª–ª–µ–∫—Ç–æ—Ä—Å–∫' in text_lower:
            adjusted_scores[4] += 0.35  # –ë–æ–Ω—É—Å –¥–ª—è ID=5
            adjusted_scores[3] -= 0.20  # –®—Ç—Ä–∞—Ñ –¥–ª—è –§–°–°–ü
            adjusted_scores[0] -= 0.10  # –®—Ç—Ä–∞—Ñ –¥–ª—è –æ–±—ã—á–Ω–æ–π –∂–∞–ª–æ–±—ã –Ω–∞ –±–∞–Ω–∫
        
        # –ü—Ä–∞–≤–∏–ª–æ 4: –§–°–°–ü/–ø—Ä–∏—Å—Ç–∞–≤ - —è–≤–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫
        if '—Ñ—Å—Å–ø' in text_lower or '–ø—Ä–∏—Å—Ç–∞–≤' in text_lower or '–∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω' in text_lower:
            if '–∫–æ–ª–ª–µ–∫—Ç–æ—Ä' not in text_lower:
                adjusted_scores[3] += 0.30  # –ë–æ–Ω—É—Å –¥–ª—è ID=4
                adjusted_scores[4] -= 0.15  # –®—Ç—Ä–∞—Ñ –¥–ª—è –∫–æ–ª–ª–µ–∫—Ç–æ—Ä–æ–≤
        
        # ===================== –ü–†–ê–í–ò–õ–ê –î–õ–Ø –ú–û–®–ï–ù–ù–ò–ß–ï–°–¢–í–ê =====================
        
        # –ü—Ä–∞–≤–∏–ª–æ 5: –ú–æ—à–µ–Ω–Ω–∏–∫–∏ + –†–æ—Å—Ñ–∏–Ω–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
        if '—Ä–æ—Å—Ñ–∏–Ω–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥' in text_lower or '—Ä—Ñ–º' in text_lower:
            # –ó–≤–æ–Ω—è—Ç –º–æ—à–µ–Ω–Ω–∏–∫–∏ –æ—Ç –∏–º–µ–Ω–∏ –†–§–ú
            if any(word in text_lower for word in ['–∑–≤–æ–Ω', '–ø–æ–∑–≤–æ–Ω–∏–ª', '–º–æ—à–µ–Ω–Ω–∏–∫', '–±–µ–∑–æ–ø–∞—Å–Ω', '–ø–µ—Ä–µ–≤–µ—Å—Ç–∏']):
                adjusted_scores[11] += 0.35  # ID=12
                # –®—Ç—Ä–∞—Ñ—É–µ–º —Å–º–µ–∂–Ω—ã–µ
                adjusted_scores[12] -= 0.10
                adjusted_scores[13] -= 0.10
                adjusted_scores[14] -= 0.10
            
            # –î–æ–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –Ω–∞ –ì–æ—Å—É—Å–ª—É–≥–∞—Ö
            if '–¥–æ–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å' in text_lower and ('–≥–æ—Å—É—Å–ª—É–≥' in text_lower or '–ø–æ—è–≤–∏–ª' in text_lower):
                adjusted_scores[12] += 0.35  # ID=13
                adjusted_scores[11] -= 0.10
            
            # –ü–∏—Å—å–º–æ –æ–± –æ–ø–ª–∞—Ç–µ
            if '–ø–∏—Å—å–º–æ' in text_lower and any(word in text_lower for word in ['–æ–ø–ª–∞—Ç', '—à—Ç—Ä–∞—Ñ', '–ø–µ–Ω–∏', '–∫–æ–º–∏—Å—Å', '–ª–∏—Ü–µ–Ω–∑']):
                adjusted_scores[13] += 0.35  # ID=14
                adjusted_scores[11] -= 0.10
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞
            if any(word in text_lower for word in ['—è–≤–ª—è–µ—Ç—Å—è –ª–∏', '–ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç—å', '–ø—Ä–æ–≤–µ—Ä–∏—Ç—å']) and '—Å–æ—Ç—Ä—É–¥–Ω–∏–∫' in text_lower:
                adjusted_scores[14] += 0.35  # ID=15
                adjusted_scores[11] -= 0.10
        
        # ===================== –ü–†–ê–í–ò–õ–ê –î–õ–Ø –ì–û–°–£–°–õ–£–ì =====================
        
        # –ü—Ä–∞–≤–∏–ª–æ 6: –ì–æ—Å—É—Å–ª—É–≥–∏ + –≤–∑–ª–æ–º
        if '–≥–æ—Å—É—Å–ª—É–≥' in text_lower or '–µ–ø–≥—É' in text_lower or '–µ—Å–∏–∞' in text_lower:
            if any(word in text_lower for word in ['–≤–∑–ª–æ–º', '–≤–∑–ª–æ–º–∞–ª', '–≤–∑–ª–æ–º–∞–ª–∏', '–ø–æ–ª—É—á–∏–ª–∏ –¥–æ—Å—Ç—É–ø', '–Ω–µ—Å–∞–Ω–∫—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω']):
                # –û–±—Ä–∞—Ç–∏–ª—Å—è –≤ –ø–æ–ª–∏—Ü–∏—é
                if any(word in text_lower for word in ['–ø–æ–ª–∏—Ü–∏', '–º–≤–¥', '–∑–∞—è–≤–ª–µ–Ω–∏', '—É–≥–æ–ª–æ–≤–Ω', '–≤–æ–∑–±—É–∂–¥–µ–Ω']):
                    adjusted_scores[17] += 0.35  # ID=18 - –æ–±—Ä–∞—Ç–∏–ª—Å—è –≤ –ø–æ–ª–∏—Ü–∏—é
                    adjusted_scores[16] -= 0.15  # –®—Ç—Ä–∞—Ñ –¥–ª—è –ø—Ä–æ—Å—Ç–æ –≤–∑–ª–æ–º–∞
                else:
                    adjusted_scores[16] += 0.30  # ID=17 - –ø—Ä–æ—Å—Ç–æ –≤–∑–ª–æ–º
                    adjusted_scores[17] -= 0.10  # –ù–µ–±–æ–ª—å—à–æ–π —à—Ç—Ä–∞—Ñ –¥–ª—è –ø–æ–ª–∏—Ü–∏–∏
            
            # –î–æ–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –Ω–∞ –ì–æ—Å—É—Å–ª—É–≥–∞—Ö
            if '–¥–æ–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å' in text_lower:
                adjusted_scores[12] += 0.30  # ID=13
        
        # ===================== –ü–†–ê–í–ò–õ–ê –î–õ–Ø –ö–†–ï–î–ò–¢–û–í =====================
        
        # –ü—Ä–∞–≤–∏–ª–æ 7: –ö—Ä–µ–¥–∏—Ç + –Ω–µ –±—Ä–∞–ª/–º–æ—à–µ–Ω–Ω–∏–∫–∏
        if '–∫—Ä–µ–¥–∏—Ç' in text_lower or '–∑–∞–π–º' in text_lower:
            if any(phrase in text_lower for phrase in ['–Ω–µ –±—Ä–∞–ª', '–Ω–µ –æ—Ñ–æ—Ä–º–ª—è–ª', '–±–µ–∑ —Å–æ–≥–ª–∞—Å–∏—è', '–±–µ–∑ –º–æ–µ–≥–æ', '–Ω–∞ –º–æ–µ –∏–º—è', '–º–æ—à–µ–Ω–Ω–∏–∫']):
                adjusted_scores[15] += 0.35  # ID=16
                adjusted_scores[0] -= 0.15  # –®—Ç—Ä–∞—Ñ –¥–ª—è –æ–±—ã—á–Ω–æ–π –∂–∞–ª–æ–±—ã –Ω–∞ –±–∞–Ω–∫
        
        # ===================== –ü–†–ê–í–ò–õ–ê –î–õ–Ø –°–ü–ï–¶–ò–ê–õ–¨–ù–´–• –°–õ–£–ß–ê–ï–í =====================
        
        # –ü—Ä–∞–≤–∏–ª–æ 8: –ú–µ–∂–≤–µ–¥–æ–º—Å—Ç–≤–µ–Ω–Ω–∞—è –∫–æ–º–∏—Å—Å–∏—è
        if '–º–µ–∂–≤–µ–¥–æ–º—Å—Ç–≤–µ–Ω–Ω' in text_lower and '–∫–æ–º–∏—Å—Å–∏' in text_lower:
            adjusted_scores[18] += 0.45  # ID=19 - –æ—á–µ–Ω—å —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π —Å–ª—É—á–∞–π
        
        # –ü—Ä–∞–≤–∏–ª–æ 9: –û—à–∏–±–æ—á–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥
        if any(phrase in text_lower for phrase in ['–æ—à–∏–±–æ—á–Ω', '—Å–ª—É—á–∞–π–Ω–æ –ø–µ—Ä–µ–≤–µ–ª', '–ø–µ—Ä–µ–ø—É—Ç–∞–ª', '–Ω–µ —Ç–æ–º—É', '–Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω']):
            if '–ø–µ—Ä–µ–≤–æ–¥' in text_lower or '–ø–µ—Ä–µ–≤–µ–ª' in text_lower:
                adjusted_scores[19] += 0.40  # ID=20
                # –®—Ç—Ä–∞—Ñ—É–µ–º –Ω–µ—Å–≤—è–∑–∞–Ω–Ω—ã–µ —Ä—É–±—Ä–∏–∫–∏
                adjusted_scores[11] -= 0.15  # –ú–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ
        
        # –ü—Ä–∞–≤–∏–ª–æ 10: –ö–∞–∑–∏–Ω–æ - —Ä–µ–∫–≤–∏–∑–∏—Ç—ã vs –∂–∞–ª–æ–±–∞
        if '–∫–∞–∑–∏–Ω–æ' in text_lower or '–±—É–∫–º–µ–∫–µ—Ä' in text_lower or '—Å—Ç–∞–≤–∫' in text_lower:
            # –†–µ–∫–≤–∏–∑–∏—Ç—ã –¥–ª—è –ø–æ–ø–æ–ª–Ω–µ–Ω–∏—è
            if any(word in text_lower for word in ['—Ä–µ–∫–≤–∏–∑–∏—Ç', '–ø–æ–ø–æ–ª–Ω–µ–Ω–∏', '–∫–∞—Ä—Ç', '—Å—á–µ—Ç']):
                if '–≤—ã–ø–ª–∞—Ç' not in text_lower and '–≤—ã–≤–æ–¥' not in text_lower:
                    adjusted_scores[9] += 0.30  # ID=10
                    adjusted_scores[10] -= 0.15
            
            # –ñ–∞–ª–æ–±–∞ –Ω–∞ –Ω–µ–≤—ã–ø–ª–∞—Ç—É
            if any(word in text_lower for word in ['–≤—ã–ø–ª–∞—Ç', '–≤—ã–≤–æ–¥', '–Ω–µ –≤—ã–ø–ª–∞—á', '–∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–ª', '–Ω–µ –º–æ–≥—É –≤—ã–≤–µ—Å—Ç–∏']):
                adjusted_scores[10] += 0.30  # ID=11
                adjusted_scores[9] -= 0.15
        
        # –ü—Ä–∞–≤–∏–ª–æ 11: –§–∏–Ω–∞–Ω—Å–æ–≤—ã–π/–∫–æ–Ω–∫—É—Ä—Å–Ω—ã–π —É–ø—Ä–∞–≤–ª—è—é—â–∏–π
        if '—É–ø—Ä–∞–≤–ª—è—é—â' in text_lower:
            if any(word in text_lower for word in ['—Ñ–∏–Ω–∞–Ω—Å–æ–≤', '–∫–æ–Ω–∫—É—Ä—Å–Ω', '–∞—Ä–±–∏—Ç—Ä–∞–∂–Ω', '–±–∞–Ω–∫—Ä–æ—Ç—Å—Ç–≤']):
                adjusted_scores[7] += 0.40  # ID=8
                adjusted_scores[8] -= 0.15  # –®—Ç—Ä–∞—Ñ –¥–ª—è –∞–¥–≤–æ–∫–∞—Ç–∞
        
        # –ü—Ä–∞–≤–∏–ª–æ 12: –ê–¥–≤–æ–∫–∞—Ç
        if '–∞–¥–≤–æ–∫–∞—Ç' in text_lower:
            if any(word in text_lower for word in ['–∑–∞–ø—Ä–æ—Å', '63-—Ñ–∑', '63 —Ñ–∑', '–¥–æ–≤–µ—Ä–∏—Ç–µ–ª']):
                adjusted_scores[8] += 0.40  # ID=9
                adjusted_scores[7] -= 0.15  # –®—Ç—Ä–∞—Ñ –¥–ª—è —É–ø—Ä–∞–≤–ª—è—é—â–µ–≥–æ
        
        # ===================== –ü–†–ê–í–ò–õ–ê –î–õ–Ø –ê–ù–¢–ò–ú–û–ù–û–ü–û–õ–¨–ù–û–ì–û/–ù–ê–õ–û–ì–û–í =====================
        
        # –ü—Ä–∞–≤–∏–ª–æ 13: –§–ê–° / –∞–Ω—Ç–∏–º–æ–Ω–æ–ø–æ–ª—å–Ω–æ–µ
        if '—Ñ–∞—Å' in text_lower or '–∞–Ω—Ç–∏–º–æ–Ω–æ–ø–æ–ª—å–Ω' in text_lower or '–º–æ–Ω–æ–ø–æ–ª' in text_lower:
            adjusted_scores[5] += 0.35  # ID=6
        
        # –ü—Ä–∞–≤–∏–ª–æ 14: –§–ù–° / –Ω–∞–ª–æ–≥–∏
        if '—Ñ–Ω—Å' in text_lower or '–Ω–∞–ª–æ–≥' in text_lower:
            if any(word in text_lower for word in ['—É–∫–ª–æ–Ω–µ–Ω', '–Ω–µ—É–ø–ª–∞—Ç', '—Å–µ—Ä–∞—è', '–∫–æ–Ω–≤–µ—Ä—Ç']):
                adjusted_scores[6] += 0.35  # ID=7
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è
        adjusted_scores = np.clip(adjusted_scores, 0, 1)
        
        return adjusted_scores
    
    def _apply_mutual_exclusion(self, scores: np.ndarray) -> np.ndarray:
        """
        –ü—Ä–∏–º–µ–Ω—è–µ—Ç –≤–∑–∞–∏–º–æ–∏—Å–∫–ª—é—á–∞—é—â–∏–µ –ø—Ä–∞–≤–∏–ª–∞ –º–µ–∂–¥—É —Ä—É–±—Ä–∏–∫–∞–º–∏.
        –ï—Å–ª–∏ –æ–¥–Ω–∞ —Ä—É–±—Ä–∏–∫–∞ —è–≤–Ω–æ –ª–∏–¥–∏—Ä—É–µ—Ç, —Å–º–µ–∂–Ω—ã–µ –ø–æ–ª—É—á–∞—é—Ç —à—Ç—Ä–∞—Ñ.
        
        Args:
            scores: —Ç–µ–∫—É—â–∏–µ –æ—Ü–µ–Ω–∫–∏
            
        Returns:
            –°–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏
        """
        adjusted_scores = scores.copy()
        
        for group in MUTUALLY_EXCLUSIVE_GROUPS:
            # –ù–∞—Ö–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å—ã —Ä—É–±—Ä–∏–∫ –≤ –≥—Ä—É–ø–ø–µ
            indices = [rubric_id - 1 for rubric_id in group]
            
            # –ù–∞—Ö–æ–¥–∏–º –ª–∏–¥–µ—Ä–∞ –≤ –≥—Ä—É–ø–ø–µ
            group_scores = [(idx, adjusted_scores[idx]) for idx in indices]
            group_scores.sort(key=lambda x: x[1], reverse=True)
            
            if len(group_scores) >= 2:
                leader_idx, leader_score = group_scores[0]
                second_idx, second_score = group_scores[1]
                
                # –ï—Å–ª–∏ –ª–∏–¥–µ—Ä –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –≤–ø–µ—Ä–µ–¥–∏, —à—Ç—Ä–∞—Ñ—É–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –≤ –≥—Ä—É–ø–ø–µ
                if leader_score > HIGH_CONFIDENCE_THRESHOLD and (leader_score - second_score) > 0.15:
                    for idx, score in group_scores[1:]:
                        # –®—Ç—Ä–∞—Ñ –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª–µ–Ω —Ä–∞–∑–Ω–∏—Ü–µ
                        penalty = min(0.20, (leader_score - score) * 0.3)
                        adjusted_scores[idx] = max(0, adjusted_scores[idx] - penalty)
        
        return adjusted_scores
    
    def predict(
        self,
        text: str,
        top_k: int = 3,
        return_scores: bool = True
    ) -> Dict:
        """
        –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∂–∞–ª–æ–±—ã.
        
        Args:
            text: —Ç–µ–∫—Å—Ç –∂–∞–ª–æ–±—ã
            top_k: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            return_scores: –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å –ª–∏ –¥–µ—Ç–∞–ª—å–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        """
        if self.model is None or self.rubric_embeddings is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ load() –∏–ª–∏ train()")
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–µ–∫—Å—Ç
        text_normalized = normalize_text(text)
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –æ—Ü–µ–Ω–∫–∏
        semantic_scores = self._calculate_semantic_scores(text)
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –æ—Ü–µ–Ω–∫–∏ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        if self.use_keywords:
            keyword_scores = self._calculate_keyword_scores(text)
            # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –æ—Ü–µ–Ω–∫–∏
            combined_scores = (
                self.semantic_weight * semantic_scores +
                self.keyword_weight * keyword_scores
            )
        else:
            keyword_scores = np.zeros(len(self.rubrics))
            combined_scores = semantic_scores
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–∞–≤–∏–ª–∞
        rule_adjusted_scores = self._apply_rule_based_adjustments(text, combined_scores)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –≤–∑–∞–∏–º–æ–∏—Å–∫–ª—é—á–∞—é—â–∏–µ –ø—Ä–∞–≤–∏–ª–∞
        final_scores = self._apply_mutual_exclusion(rule_adjusted_scores)
        
        # –ù–∞—Ö–æ–¥–∏–º —Ç–æ–ø-k —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        top_indices = np.argsort(final_scores)[::-1][:top_k]
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π
        predictions = []
        best_score = final_scores[top_indices[0]] if len(top_indices) > 0 else 0
        
        for i, idx in enumerate(top_indices):
            rubric = self.rubrics[idx]
            rubric_id = rubric['id']
            score = float(final_scores[idx])
            
            # –î–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ (–Ω–µ –ø–µ—Ä–≤—ã–π) –ø—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é
            if i > 0:
                # –§–∏–ª—å—Ç—Ä 1: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ confidence
                if score < MIN_CONFIDENCE_THRESHOLD:
                    continue
                
                # –§–∏–ª—å—Ç—Ä 2: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑—Ä—ã–≤ —Å –ª–∏–¥–µ—Ä–æ–º
                gap = best_score - score
                if gap > MAX_GAP_THRESHOLD:
                    continue
                
                # –§–∏–ª—å—Ç—Ä 3: –ï—Å–ª–∏ –ª–∏–¥–µ—Ä –æ—á–µ–Ω—å —É–≤–µ—Ä–µ–Ω–Ω—ã–π, —Å—Ç—Ä–æ–∂–µ —Ñ–∏–ª—å—Ç—Ä—É–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã—Ö
                if best_score > HIGH_CONFIDENCE_THRESHOLD and score < MIN_CONFIDENCE_THRESHOLD + 0.10:
                    continue
            
            predictions.append({
                'rubric_id': rubric_id,
                'rubric_name': rubric['description'],  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
                'short_name': rubric['name'],  # –ö—Ä–∞—Ç–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–ª—è —Å–ø—Ä–∞–≤–∫–∏
                'response_template': get_response_template(rubric_id),  # –®–∞–±–ª–æ–Ω –æ—Ç–≤–µ—Ç–∞
                'confidence': score,
                'semantic_score': float(semantic_scores[idx]) if return_scores else None,
                'keyword_score': float(keyword_scores[idx]) if return_scores else None
            })
        
        result = {
            'text': text,
            'predictions': predictions,
            'best_match': predictions[0] if predictions else None,
            'filtered_count': top_k - len(predictions)  # –°–∫–æ–ª—å–∫–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ
        }
        
        return result
    
    def _filter_predictions(
        self,
        predictions: List[Dict],
        min_confidence: float = MIN_CONFIDENCE_THRESHOLD,
        max_gap: float = MAX_GAP_THRESHOLD
    ) -> List[Dict]:
        """
        –§–∏–ª—å—Ç—Ä—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π.
        
        Args:
            predictions: —Å–ø–∏—Å–æ–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
            min_confidence: –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ confidence
            max_gap: –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑—Ä—ã–≤ —Å –ª–∏–¥–µ—Ä–æ–º
            
        Returns:
            –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫
        """
        if not predictions:
            return predictions
        
        best_score = predictions[0]['confidence']
        filtered = [predictions[0]]  # –õ–∏–¥–µ—Ä –≤—Å–µ–≥–¥–∞ –æ—Å—Ç–∞—ë—Ç—Å—è
        
        for pred in predictions[1:]:
            score = pred['confidence']
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥
            if score < min_confidence:
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑—Ä—ã–≤ —Å –ª–∏–¥–µ—Ä–æ–º
            if (best_score - score) > max_gap:
                continue
            
            # –ï—Å–ª–∏ –ª–∏–¥–µ—Ä –æ—á–µ–Ω—å —É–≤–µ—Ä–µ–Ω–Ω—ã–π, —Å—Ç—Ä–æ–∂–µ —Ñ–∏–ª—å—Ç—Ä—É–µ–º
            if best_score > HIGH_CONFIDENCE_THRESHOLD and score < min_confidence + 0.10:
                continue
            
            filtered.append(pred)
        
        return filtered
    
    def get_cache_stats(self) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫—ç—à–∞"""
        return self.embedding_cache.get_stats()
    
    def clear_cache(self):
        """–û—á–∏—â–∞–µ—Ç –∫—ç—à —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        self.embedding_cache.clear()
    
    def predict_batch(
        self,
        texts: List[str],
        top_k: int = 1
    ) -> List[Dict]:
        """
        –ü–∞–∫–µ—Ç–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∂–∞–ª–æ–±.
        –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å batch-–æ–±—Ä–∞–±–æ—Ç–∫–æ–π —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.
        
        Args:
            texts: —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –∂–∞–ª–æ–±
            top_k: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        """
        if not texts:
            return []
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏ –Ω–µ–∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ
        uncached_texts = []
        uncached_indices = []
        cached_embeddings = {}
        
        for i, text in enumerate(texts):
            cached = self.embedding_cache.get(text)
            if cached is not None:
                cached_embeddings[i] = cached
            else:
                uncached_texts.append(text)
                uncached_indices.append(i)
        
        # Batch-–æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ–∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
        if uncached_texts:
            new_embeddings = self.model.encode(
                uncached_texts,
                convert_to_numpy=True,
                show_progress_bar=len(uncached_texts) > 10
            )
            
            # –ö—ç—à–∏—Ä—É–µ–º –Ω–æ–≤—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
            for i, (text, embedding) in enumerate(zip(uncached_texts, new_embeddings)):
                orig_idx = uncached_indices[i]
                cached_embeddings[orig_idx] = embedding.reshape(1, -1)
                self.embedding_cache.put(text, embedding.reshape(1, -1))
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Ç–µ–∫—Å—Ç
        results = []
        for i, text in enumerate(texts):
            result = self.predict(text, top_k=top_k, return_scores=False)
            results.append(result)
        
        return results
