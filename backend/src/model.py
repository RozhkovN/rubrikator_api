"""
–û—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –∂–∞–ª–æ–±.

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥:
1. Sentence Transformers –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
2. –ê–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è —É—Ç–æ—á–Ω–µ–Ω–∏—è
3. –ü—Ä–∏–º–µ—Ä—ã –∂–∞–ª–æ–± –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
4. –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –∏ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
"""

import os
import pickle
import numpy as np
from typing import List, Dict, Tuple, Optional
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

from config.rubrics import RUBRICS, get_rubric_by_id
from config.response_templates import get_response_template
from src.preprocessor import (
    normalize_text, 
    calculate_keyword_score,
    calculate_advanced_keyword_score,
    extract_law_references,
    extract_organization_mentions
)


class ComplaintClassifier:
    """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∂–∞–ª–æ–± –Ω–∞ –æ—Å–Ω–æ–≤–µ Sentence Transformers"""
    
    def __init__(
        self,
        model_name: str = "paraphrase-multilingual-mpnet-base-v2",
        use_keywords: bool = True,
        keyword_weight: float = 0.35,
        use_examples: bool = True
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞.
        
        Args:
            model_name: –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ Sentence Transformers
            use_keywords: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ –∞–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            keyword_weight: –≤–µ—Å –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ (0-1), –æ—Å—Ç–∞–ª—å–Ω–æ–µ - —Å–µ–º–∞–Ω—Ç–∏–∫–∞
            use_examples: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        """
        self.model_name = model_name
        self.use_keywords = use_keywords
        self.keyword_weight = keyword_weight
        self.semantic_weight = 1 - keyword_weight
        self.use_examples = use_examples
        
        self.model: Optional[SentenceTransformer] = None
        self.rubric_embeddings: Optional[np.ndarray] = None
        self.example_embeddings: Optional[Dict[int, np.ndarray]] = None
        self.rubrics = RUBRICS
        
        print(f"üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞...")
        print(f"   –ú–æ–¥–µ–ª—å: {model_name}")
        print(f"   –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {'–î–∞' if use_keywords else '–ù–µ—Ç'}")
        print(f"   –ü—Ä–∏–º–µ—Ä—ã: {'–î–∞' if use_examples else '–ù–µ—Ç'}")
        if use_keywords:
            print(f"   –í–µ—Å–∞: —Å–µ–º–∞–Ω—Ç–∏–∫–∞={self.semantic_weight:.2f}, –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞={self.keyword_weight:.2f}")
    
    def load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Sentence Transformers"""
        if self.model is None:
            print(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {self.model_name}...")
            self.model = SentenceTransformer(self.model_name)
            print("‚úì –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    
    def prepare_rubric_texts(self) -> List[str]:
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π —Ä—É–±—Ä–∏–∫–∞—Ç–æ—Ä–æ–≤.
        –£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏.
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
        """
        texts = []
        for rubric in self.rubrics:
            # –ë–∞–∑–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
            parts = [rubric['description']]
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            parts.append(f"–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {', '.join(rubric['keywords'])}")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            if 'priority_keywords' in rubric and rubric['priority_keywords']:
                parts.append(f"–í–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {', '.join(rubric['priority_keywords'])}")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            if self.use_examples and 'examples' in rubric and rubric['examples']:
                examples_text = " | ".join(rubric['examples'][:3])  # –ë–µ—Ä–µ–º –¥–æ 3 –ø—Ä–∏–º–µ—Ä–æ–≤
                parts.append(f"–ü—Ä–∏–º–µ—Ä—ã –æ–±—Ä–∞—â–µ–Ω–∏–π: {examples_text}")
            
            text = ". ".join(parts)
            texts.append(text)
        
        return texts
    
    def train(self, save_path: str = "models/classifier.pkl"):
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ (—Å–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ä—É–±—Ä–∏–∫–∞—Ç–æ—Ä–æ–≤).
        
        Args:
            save_path: –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        """
        print("\nüéØ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å, –µ—Å–ª–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞
        self.load_model()
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ä—É–±—Ä–∏–∫–∞—Ç–æ—Ä–æ–≤
        print("üìù –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ–ø–∏—Å–∞–Ω–∏–π —Ä—É–±—Ä–∏–∫–∞—Ç–æ—Ä–æ–≤...")
        rubric_texts = self.prepare_rubric_texts()
        
        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ä—É–±—Ä–∏–∫–∞—Ç–æ—Ä–æ–≤
        print("üîÑ –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π —Ä—É–±—Ä–∏–∫–∞—Ç–æ—Ä–æ–≤...")
        self.rubric_embeddings = self.model.encode(
            rubric_texts,
            show_progress_bar=True,
            convert_to_numpy=True
        )
        
        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä—É–±—Ä–∏–∫–∞—Ç–æ—Ä–∞
        self.example_embeddings = {}
        if self.use_examples:
            print("üîÑ –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –ø—Ä–∏–º–µ—Ä–æ–≤...")
            for rubric in self.rubrics:
                if 'examples' in rubric and rubric['examples']:
                    embeddings = self.model.encode(
                        rubric['examples'],
                        convert_to_numpy=True
                    )
                    self.example_embeddings[rubric['id']] = embeddings
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        with open(save_path, 'wb') as f:
            pickle.dump({
                'embeddings': self.rubric_embeddings,
                'example_embeddings': self.example_embeddings,
                'model_name': self.model_name,
                'use_keywords': self.use_keywords,
                'keyword_weight': self.keyword_weight,
                'use_examples': self.use_examples
            }, f)
        
        print(f"‚úì –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {save_path}")
        print(f"‚úì –°–æ–∑–¥–∞–Ω–æ {len(self.rubric_embeddings)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ä—É–±—Ä–∏–∫–∞—Ç–æ—Ä–æ–≤")
        print(f"‚úì –°–æ–∑–¥–∞–Ω–æ {len(self.example_embeddings)} –Ω–∞–±–æ—Ä–æ–≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    def load(self, load_path: str = "models/classifier.pkl"):
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞.
        
        Args:
            load_path: –ø—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        """
        print(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –∏–∑ {load_path}...")
        
        with open(load_path, 'rb') as f:
            data = pickle.load(f)
        
        self.rubric_embeddings = data['embeddings']
        self.example_embeddings = data.get('example_embeddings', {})
        self.model_name = data['model_name']
        self.use_keywords = data.get('use_keywords', True)
        self.keyword_weight = data.get('keyword_weight', 0.35)
        self.use_examples = data.get('use_examples', True)
        self.semantic_weight = 1 - self.keyword_weight
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        self.load_model()
        
        print("‚úì –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω")
    
    def _calculate_semantic_scores(self, text: str) -> np.ndarray:
        """
        –†–∞—Å—á–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Ü–µ–Ω–æ–∫ —á–µ—Ä–µ–∑ cosine similarity.
        –£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å —É—á–µ—Ç–æ–º –ø—Ä–∏–º–µ—Ä–æ–≤.
        
        Args:
            text: —Ç–µ–∫—Å—Ç –∂–∞–ª–æ–±—ã
            
        Returns:
            –ú–∞—Å—Å–∏–≤ –æ—Ü–µ–Ω–æ–∫ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä—É–±—Ä–∏–∫–∞—Ç–æ—Ä–∞
        """
        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ —Ç–µ–∫—Å—Ç–∞
        text_embedding = self.model.encode([text], convert_to_numpy=True)
        
        # –°—á–∏—Ç–∞–µ–º cosine similarity —Å –æ–ø–∏—Å–∞–Ω–∏—è–º–∏ —Ä—É–±—Ä–∏–∫–∞—Ç–æ—Ä–æ–≤
        rubric_similarities = cosine_similarity(text_embedding, self.rubric_embeddings)[0]
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤ –¥–∏–∞–ø–∞–∑–æ–Ω [0, 1]
        scores = (rubric_similarities + 1) / 2
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏
        if self.use_examples and self.example_embeddings:
            for rubric in self.rubrics:
                rubric_id = rubric['id']
                idx = rubric_id - 1  # ID –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å 1
                
                if rubric_id in self.example_embeddings:
                    example_emb = self.example_embeddings[rubric_id]
                    example_sim = cosine_similarity(text_embedding, example_emb)[0]
                    
                    # –ë–µ—Ä–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏
                    max_example_sim = (np.max(example_sim) + 1) / 2
                    
                    # –°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞ –º–µ–∂–¥—É –æ–ø–∏—Å–∞–Ω–∏–µ–º –∏ –ª—É—á—à–∏–º –ø—Ä–∏–º–µ—Ä–æ–º
                    # –° –Ω–µ–±–æ–ª—å—à–∏–º –±–æ–Ω—É—Å–æ–º –∑–∞ –ø—Ä–∏–º–µ—Ä—ã (–æ–Ω–∏ –±–æ–ª–µ–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ)
                    scores[idx] = 0.4 * scores[idx] + 0.6 * max_example_sim
        
        return scores
    
    def _calculate_keyword_scores(self, text: str) -> np.ndarray:
        """
        –†–∞—Å—á–µ—Ç –æ—Ü–µ–Ω–æ–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤.
        –£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏ –∏ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–º–∏ —Å–ª–æ–≤–∞–º–∏.
        
        Args:
            text: —Ç–µ–∫—Å—Ç –∂–∞–ª–æ–±—ã
            
        Returns:
            –ú–∞—Å—Å–∏–≤ –æ—Ü–µ–Ω–æ–∫ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä—É–±—Ä–∏–∫–∞—Ç–æ—Ä–∞
        """
        scores = np.zeros(len(self.rubrics))
        
        for i, rubric in enumerate(self.rubrics):
            score, _ = calculate_advanced_keyword_score(
                text,
                rubric['keywords'],
                rubric.get('priority_keywords', []),
                rubric.get('negative_keywords', [])
            )
            scores[i] = score
        
        return scores
    
    def _apply_rule_based_adjustments(self, text: str, scores: np.ndarray) -> np.ndarray:
        """
        –ü—Ä–∏–º–µ–Ω—è–µ—Ç –ø—Ä–∞–≤–∏–ª–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤.
        
        Args:
            text: —Ç–µ–∫—Å—Ç –∂–∞–ª–æ–±—ã
            scores: —Ç–µ–∫—É—â–∏–µ –æ—Ü–µ–Ω–∫–∏
            
        Returns:
            –°–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏
        """
        text_lower = text.lower()
        adjusted_scores = scores.copy()
        
        # –ü—Ä–∞–≤–∏–ª–æ 1: –ï—Å–ª–∏ –µ—Å—Ç—å 161-–§–ó - —ç—Ç–æ —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ (ID=2)
        if '161-—Ñ–∑' in text_lower or '161 —Ñ–∑' in text_lower:
            adjusted_scores[1] += 0.25  # –ë–æ–Ω—É—Å –¥–ª—è ID=2
            adjusted_scores[0] -= 0.1   # –®—Ç—Ä–∞—Ñ –¥–ª—è ID=1
        
        # –ü—Ä–∞–≤–∏–ª–æ 2: –ï—Å–ª–∏ –µ—Å—Ç—å 115-–§–ó –±–µ–∑ —É–ø—Ä–∞–≤–ª—è—é—â–µ–≥–æ/–∞–¥–≤–æ–∫–∞—Ç–∞ - —ç—Ç–æ ID=3
        if ('115-—Ñ–∑' in text_lower or '115 —Ñ–∑' in text_lower):
            if '—É–ø—Ä–∞–≤–ª—è—é—â' not in text_lower and '–∞–¥–≤–æ–∫–∞—Ç' not in text_lower:
                adjusted_scores[2] += 0.2  # –ë–æ–Ω—É—Å –¥–ª—è ID=3
        
        # –ü—Ä–∞–≤–∏–ª–æ 3: –ö–æ–ª–ª–µ–∫—Ç–æ—Ä—ã - —è–≤–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫
        if '–∫–æ–ª–ª–µ–∫—Ç–æ—Ä' in text_lower:
            adjusted_scores[4] += 0.3  # –ë–æ–Ω—É—Å –¥–ª—è ID=5
            adjusted_scores[3] -= 0.15  # –®—Ç—Ä–∞—Ñ –¥–ª—è –§–°–°–ü
        
        # –ü—Ä–∞–≤–∏–ª–æ 4: –§–°–°–ü/–ø—Ä–∏—Å—Ç–∞–≤ - —è–≤–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫
        if '—Ñ—Å—Å–ø' in text_lower or '–ø—Ä–∏—Å—Ç–∞–≤' in text_lower:
            if '–∫–æ–ª–ª–µ–∫—Ç–æ—Ä' not in text_lower:
                adjusted_scores[3] += 0.25  # –ë–æ–Ω—É—Å –¥–ª—è ID=4
        
        # –ü—Ä–∞–≤–∏–ª–æ 5: –ú–æ—à–µ–Ω–Ω–∏–∫–∏ + –†–æ—Å—Ñ–∏–Ω–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ + –∑–≤–æ–Ω–æ–∫ = ID=12
        if '—Ä–æ—Å—Ñ–∏–Ω–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥' in text_lower:
            if '–∑–≤–æ–Ω' in text_lower or '–ø–æ–∑–≤–æ–Ω–∏–ª' in text_lower or '–º–æ—à–µ–Ω–Ω–∏–∫' in text_lower:
                adjusted_scores[11] += 0.3  # ID=12
            if '–¥–æ–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å' in text_lower:
                adjusted_scores[12] += 0.3  # ID=13
            if '–ø–∏—Å—å–º–æ' in text_lower and ('–æ–ø–ª–∞—Ç' in text_lower or '—à—Ç—Ä–∞—Ñ' in text_lower):
                adjusted_scores[13] += 0.3  # ID=14
            if '—Å–æ—Ç—Ä—É–¥–Ω–∏–∫' in text_lower and '–ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç—å' in text_lower:
                adjusted_scores[14] += 0.3  # ID=15
        
        # –ü—Ä–∞–≤–∏–ª–æ 6: –ì–æ—Å—É—Å–ª—É–≥–∏ + –≤–∑–ª–æ–º
        if '–≥–æ—Å—É—Å–ª—É–≥' in text_lower or '–µ–ø–≥—É' in text_lower:
            if '–≤–∑–ª–æ–º' in text_lower or '–≤–∑–ª–æ–º–∞–ª' in text_lower:
                if '–ø–æ–ª–∏—Ü–∏' in text_lower or '–º–≤–¥' in text_lower or '–∑–∞—è–≤–ª–µ–Ω–∏' in text_lower:
                    adjusted_scores[17] += 0.25  # ID=18 - –æ–±—Ä–∞—Ç–∏–ª—Å—è –≤ –ø–æ–ª–∏—Ü–∏—é
                else:
                    adjusted_scores[16] += 0.25  # ID=17 - –ø—Ä–æ—Å—Ç–æ –≤–∑–ª–æ–º
            if '–¥–æ–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å' in text_lower:
                adjusted_scores[12] += 0.25  # ID=13
        
        # –ü—Ä–∞–≤–∏–ª–æ 7: –ö—Ä–µ–¥–∏—Ç + –Ω–µ –±—Ä–∞–ª/–º–æ—à–µ–Ω–Ω–∏–∫–∏
        if '–∫—Ä–µ–¥–∏—Ç' in text_lower:
            if '–Ω–µ –±—Ä–∞–ª' in text_lower or '–Ω–µ –æ—Ñ–æ—Ä–º–ª—è–ª' in text_lower or '–±–µ–∑ —Å–æ–≥–ª–∞—Å–∏—è' in text_lower:
                adjusted_scores[15] += 0.3  # ID=16
        
        # –ü—Ä–∞–≤–∏–ª–æ 8: –ú–µ–∂–≤–µ–¥–æ–º—Å—Ç–≤–µ–Ω–Ω–∞—è –∫–æ–º–∏—Å—Å–∏—è
        if '–º–µ–∂–≤–µ–¥–æ–º—Å—Ç–≤–µ–Ω–Ω' in text_lower and '–∫–æ–º–∏—Å—Å–∏' in text_lower:
            adjusted_scores[18] += 0.4  # ID=19
        
        # –ü—Ä–∞–≤–∏–ª–æ 9: –û—à–∏–±–æ—á–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥
        if '–æ—à–∏–±–æ—á–Ω' in text_lower and '–ø–µ—Ä–µ–≤–æ–¥' in text_lower:
            adjusted_scores[19] += 0.35  # ID=20
        if '–ø–µ—Ä–µ–ø—É—Ç–∞–ª' in text_lower and ('–Ω–æ–º–µ—Ä' in text_lower or '—Ä–µ–∫–≤–∏–∑–∏—Ç' in text_lower):
            adjusted_scores[19] += 0.25  # ID=20
        
        # –ü—Ä–∞–≤–∏–ª–æ 10: –ö–∞–∑–∏–Ω–æ
        if '–∫–∞–∑–∏–Ω–æ' in text_lower:
            if '—Ä–µ–∫–≤–∏–∑–∏—Ç' in text_lower or '–ø–æ–ø–æ–ª–Ω–µ–Ω–∏' in text_lower:
                adjusted_scores[9] += 0.25  # ID=10
            if '–≤—ã–ø–ª–∞—Ç' in text_lower or '–≤—ã–≤–æ–¥' in text_lower or '–Ω–µ –≤—ã–ø–ª–∞—á' in text_lower:
                adjusted_scores[10] += 0.25  # ID=11
        
        # –ü—Ä–∞–≤–∏–ª–æ 11: –§–∏–Ω–∞–Ω—Å–æ–≤—ã–π/–∫–æ–Ω–∫—É—Ä—Å–Ω—ã–π —É–ø—Ä–∞–≤–ª—è—é—â–∏–π
        if '—É–ø—Ä–∞–≤–ª—è—é—â' in text_lower:
            if '—Ñ–∏–Ω–∞–Ω—Å–æ–≤' in text_lower or '–∫–æ–Ω–∫—É—Ä—Å–Ω' in text_lower or '–∞—Ä–±–∏—Ç—Ä–∞–∂–Ω' in text_lower:
                adjusted_scores[7] += 0.35  # ID=8
        
        # –ü—Ä–∞–≤–∏–ª–æ 12: –ê–¥–≤–æ–∫–∞—Ç
        if '–∞–¥–≤–æ–∫–∞—Ç' in text_lower and ('–∑–∞–ø—Ä–æ—Å' in text_lower or '63-—Ñ–∑' in text_lower):
            adjusted_scores[8] += 0.35  # ID=9
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è
        adjusted_scores = np.clip(adjusted_scores, 0, 1)
        
        return adjusted_scores
    
    def predict(
        self,
        text: str,
        top_k: int = 3,
        return_scores: bool = True
    ) -> Dict:
        """
        –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∂–∞–ª–æ–±—ã.
        
        Args:
            text: —Ç–µ–∫—Å—Ç –∂–∞–ª–æ–±—ã
            top_k: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            return_scores: –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å –ª–∏ –¥–µ—Ç–∞–ª—å–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        """
        if self.model is None or self.rubric_embeddings is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ load() –∏–ª–∏ train()")
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–µ–∫—Å—Ç
        text_normalized = normalize_text(text)
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –æ—Ü–µ–Ω–∫–∏
        semantic_scores = self._calculate_semantic_scores(text)
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –æ—Ü–µ–Ω–∫–∏ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        if self.use_keywords:
            keyword_scores = self._calculate_keyword_scores(text)
            # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –æ—Ü–µ–Ω–∫–∏
            combined_scores = (
                self.semantic_weight * semantic_scores +
                self.keyword_weight * keyword_scores
            )
        else:
            keyword_scores = np.zeros(len(self.rubrics))
            combined_scores = semantic_scores
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–∞–≤–∏–ª–∞
        final_scores = self._apply_rule_based_adjustments(text, combined_scores)
        
        # –ù–∞—Ö–æ–¥–∏–º —Ç–æ–ø-k —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        top_indices = np.argsort(final_scores)[::-1][:top_k]
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        predictions = []
        for idx in top_indices:
            rubric = self.rubrics[idx]
            rubric_id = rubric['id']
            predictions.append({
                'rubric_id': rubric_id,
                'rubric_name': rubric['description'],  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
                'short_name': rubric['name'],  # –ö—Ä–∞—Ç–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–ª—è —Å–ø—Ä–∞–≤–∫–∏
                'response_template': get_response_template(rubric_id),  # –®–∞–±–ª–æ–Ω –æ—Ç–≤–µ—Ç–∞
                'confidence': float(final_scores[idx]),
                'semantic_score': float(semantic_scores[idx]) if return_scores else None,
                'keyword_score': float(keyword_scores[idx]) if return_scores else None
            })
        
        result = {
            'text': text,
            'predictions': predictions,
            'best_match': predictions[0] if predictions else None
        }
        
        return result
    
    def predict_batch(
        self,
        texts: List[str],
        top_k: int = 1
    ) -> List[Dict]:
        """
        –ü–∞–∫–µ—Ç–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∂–∞–ª–æ–±.
        
        Args:
            texts: —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –∂–∞–ª–æ–±
            top_k: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        """
        results = []
        for text in texts:
            result = self.predict(text, top_k=top_k, return_scores=False)
            results.append(result)
        return results
